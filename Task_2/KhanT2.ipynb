{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn import linear_model\n",
    "from scipy import special\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(69) #fixing seed for reproducability              "
   ]
  },
  {
   "source": [
    "To generate the final table. Values must be binary. cutn ass mutherfocker\n",
    "use the 12 entries of heartrate to regress to the label asked. try linear first then go poly. ignore drugs for now\n",
    "\n",
    "## TASK 0 #######################################################################################################\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vitals_labels = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "Test_labels = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "Vitals = ['RRate', 'ABPm', 'SpO2', 'Heartrate']\n",
    "Test = ['BaseExcess', 'Fibrinogen', 'AST', 'Alkalinephos', 'Bilirubin_total',\n",
    "         'Lactate', 'TroponinI', 'SaO2',\n",
    "         'Bilirubin_direct', 'EtCO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 94.  99.  92. ...  66.  67.  69.]\n [  0.  82.  83. ...  76.  74.  75.]\n [ 94.  91.  94. ... 113. 107. 113.]\n ...\n [ 86.  94.  95. ... 100.  96.  95.]\n [  0. 106. 106. ... 101.  96. 100.]\n [  0.  76.  75. ...  80.  75.  79.]]\n[[100. 100. 100. ... 100. 100. 100.]\n [  0.  99.  98. ...  99.  99.  98.]\n [100.  99.  95. ...  96.  97.  97.]\n ...\n [100. 100.  99. ... 100. 100. 100.]\n [  0. 100. 100. ... 100. 100. 100.]\n [  0.  96.  99. ...  98.  98.   0.]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_features.csv\")\n",
    "df_train= df.groupby(['pid'],sort=False).mean()\n",
    "df_train_test = df_train[Test]\n",
    "df_train_vitals=df_train[Vitals]\n",
    "df_final=pd.concat([df_train_test,df_train_vitals],axis=1)\n",
    "#df_final = df_final.fillna(0)\n",
    "#print(df_final)\n",
    "df_final = df_final.notnull().astype('int')\n",
    "#print(df_final)\n",
    "\n",
    "labels = pd.read_csv(\"train_labels.csv\")\n",
    "labels_sebsis = labels[[\"LABEL_Sepsis\"]].to_numpy()\n",
    "labels_hr = labels[[\"LABEL_Heartrate\"]].to_numpy()\n",
    "labels_spo2 = labels[[\"LABEL_SpO2\"]].to_numpy()\n",
    "labels_abpm = labels[[\"LABEL_ABPm\"]].to_numpy()\n",
    "labels_rrate = labels[[\"LABEL_RRate\"]].to_numpy()\n",
    "#scaler0 = skl.preprocessing.StandardScaler()\n",
    "#scaler0.fit(labels)\n",
    "#labels = scaler0.transform(labels)\n",
    "heartrate = df[[\"Heartrate\"]].to_numpy()\n",
    "rrate = df[[\"RRate\"]].to_numpy()\n",
    "abps = df[[\"ABPs\"]].to_numpy()\n",
    "abpm = df[[\"ABPm\"]].to_numpy() \n",
    "spo2 = df[[\"SpO2\"]].to_numpy()\n",
    "\n",
    "heartrate = heartrate.reshape((18995,12))\n",
    "rrate = rrate.reshape((18995,12))\n",
    "abps = abps.reshape((18995,12))\n",
    "abpm = abpm.reshape((18995,12))\n",
    "spo2 = spo2.reshape((18995,12))\n",
    "\n",
    "\n",
    "#lets find what stupid patient doesnt have a heartrate\n",
    "\n",
    "\n",
    "#cause some fuckers didnt get a measurement.\n",
    "heartrate = np.nan_to_num(heartrate,nan=0)\n",
    "rrate = np.nan_to_num(rrate,nan=0)\n",
    "abps = np.nan_to_num(abps,nan=0)\n",
    "abpm = np.nan_to_num(abpm,nan=0)\n",
    "spo2 = np.nan_to_num(spo2,nan=0)\n",
    "\n",
    "#print(heartrate)\n",
    "#print(spo2)\n",
    "\n",
    "#lets use concatenated full data set TRAIN DATA\n",
    "fit_df = df_train.fillna(0).to_numpy()\n",
    "fit_df = np.delete(fit_df,0,1) #remove patient ID\n",
    "fit_df = np.delete(fit_df,0,1) #remove time spent in ICU\n",
    "\n",
    "scaler1 = skl.preprocessing.StandardScaler()\n",
    "scaler1.fit(fit_df)\n",
    "fit_df = scaler1.transform(fit_df)\n",
    "\n",
    "scaler2 = skl.preprocessing.StandardScaler()\n",
    "scaler2.fit(heartrate)\n",
    "heartrate = scaler2.transform(heartrate)\n",
    "\n",
    "scaler3 = skl.preprocessing.StandardScaler()\n",
    "scaler3.fit(rrate)\n",
    "rrate = scaler3.transform(rrate)\n",
    "\n",
    "scaler4 = skl.preprocessing.StandardScaler()\n",
    "scaler4.fit(abps)\n",
    "abps = scaler4.transform(abps)\n",
    "\n",
    "scaler5 = skl.preprocessing.StandardScaler()\n",
    "scaler5.fit(abpm)\n",
    "abpm = scaler5.transform(abpm)\n",
    "\n",
    "scaler6 = skl.preprocessing.StandardScaler()\n",
    "scaler6.fit(spo2)\n",
    "spo2 = scaler6.transform(spo2)\n",
    "\n",
    "fit_df = np.concatenate((fit_df, heartrate, rrate, abps, abpm, spo2), axis=1)\n",
    "\n",
    "#SAME FOR TEST DATA\n",
    "test_df_full = pd.read_csv(\"test_features.csv\")\n",
    "test_df = test_df_full.groupby(['pid'],sort=False).mean()\n",
    "test_df = test_df.fillna(0).to_numpy()\n",
    "pid_test = test_df_full[[\"pid\"]].to_numpy()\n",
    "pid_test = pd.DataFrame({'pid': pid_test[:,0]}).drop_duplicates().reset_index(drop=True)\n",
    "test_df = np.delete(test_df,0,1) #remove patient ID\n",
    "test_df = np.delete(test_df,0,1) #remove time spent in ICU\n",
    "labels_test = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "heartrate = test_df_full[[\"Heartrate\"]].fillna(0).to_numpy()\n",
    "heartrate = heartrate.reshape((12664,12))\n",
    "\n",
    "rrate = test_df_full[[\"RRate\"]].fillna(0).to_numpy()\n",
    "rrate = rrate.reshape((12664,12))\n",
    "\n",
    "abps = test_df_full[[\"ABPs\"]].fillna(0).to_numpy()\n",
    "abps = abps.reshape((12664,12))\n",
    "\n",
    "abpm = test_df_full[[\"ABPm\"]].fillna(0).to_numpy()\n",
    "abpm = abpm.reshape((12664,12))\n",
    "\n",
    "spo2 = test_df_full[[\"SpO2\"]].fillna(0).to_numpy()\n",
    "spo2 = spo2.reshape((12664,12))\n",
    "\n",
    "test_df = scaler1.transform(test_df)\n",
    "heartrate = scaler2.transform(heartrate)\n",
    "rrate = scaler3.transform(rrate)\n",
    "abps = scaler4.transform(abps)\n",
    "abpm = scaler5.transform(abpm)\n",
    "spo2 = scaler6.transform(spo2)\n",
    "\n",
    "test_df = np.concatenate((test_df, heartrate, rrate, abps, abpm, spo2), axis=1)"
   ]
  },
  {
   "source": [
    "#TESTusing localoutliers - create mask for training data\n",
    "#something = np.array([10,20, 50,100,150,200])\n",
    "#for item in something:\n",
    "#    value = item\n",
    "#    lof = skl.neighbors.LocalOutlierFactor(n_neighbors=value)  # try out different factors\n",
    "#    yhat = lof.fit_predict(fit_df)\n",
    "#    mask = yhat != -1\n",
    "#    longmask = np.repeat(mask, 12, axis=0)\n",
    "#    train_vital_means = df_train[Vitals]\n",
    "#    print(np.count_nonzero(mask))\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16273\n"
     ]
    }
   ],
   "source": [
    "#using localoutliers - create mask for training data\n",
    "lof = skl.neighbors.LocalOutlierFactor(n_neighbors= 150) #try out different factors\n",
    "yhat = lof.fit_predict(fit_df)\n",
    "\n",
    "mask = yhat != -1\n",
    "longmask = np.repeat(mask,12,axis=0)\n",
    "\n",
    "\n",
    "train_vital_means = df_train[Vitals]\n",
    "\n",
    "print(np.count_nonzero(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers from data\n",
    "fit_df, labels_hr, labels_spo2, labels_abpm, labels_rrate, labels_sebsis = \\\n",
    "    fit_df[mask,:], labels_hr[mask], labels_spo2[mask], labels_abpm[mask], labels_rrate[mask], labels_sebsis[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand fit_df with the vitals asked for in the exercise\n",
    "append_rrate = df[[\"RRate\"]].to_numpy()\n",
    "append_rrate = np.nan_to_num(append_rrate,nan=0)\n",
    "append_rrate = append_rrate[longmask,:].reshape(np.count_nonzero(mask),12)\n",
    "append_abpm = df[[\"ABPm\"]].to_numpy()\n",
    "append_abpm = np.nan_to_num(append_abpm,nan=0)\n",
    "append_abpm = append_abpm[longmask,:].reshape(np.count_nonzero(mask),12)\n",
    "append_sp02 = df[[\"SpO2\"]].to_numpy()\n",
    "append_sp02 = np.nan_to_num(append_sp02,nan=0)\n",
    "append_sp02 = append_sp02[longmask,:].reshape(np.count_nonzero(mask),12)\n",
    "\n",
    "scaler_rrate = skl.preprocessing.StandardScaler()\n",
    "scaler_rrate.fit(append_rrate)\n",
    "append_rrate = scaler_rrate.transform(append_rrate)\n",
    "\n",
    "scaler_abpm = skl.preprocessing.StandardScaler()\n",
    "scaler_abpm.fit(append_abpm)\n",
    "append_abpm = scaler_abpm.transform(append_abpm)\n",
    "\n",
    "scaler_sp02 = skl.preprocessing.StandardScaler()\n",
    "scaler_sp02.fit(append_sp02)\n",
    "append_sp02 = scaler_sp02.transform(append_sp02)\n",
    "\n",
    "fit_df = np.concatenate((fit_df, append_rrate, append_abpm, append_sp02), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same shit with the test_data fuuuuuuuuuuuuuuck\n",
    "append_rrate = test_df_full[[\"RRate\"]].to_numpy()\n",
    "append_rrate = np.nan_to_num(append_rrate,nan=0)\n",
    "append_rrate = append_rrate.reshape(12664,12)\n",
    "append_abpm = test_df_full[[\"ABPm\"]].to_numpy()\n",
    "append_abpm = np.nan_to_num(append_abpm,nan=0)\n",
    "append_abpm = append_abpm.reshape(12664,12)\n",
    "append_sp02 = test_df_full[[\"SpO2\"]].to_numpy()\n",
    "append_sp02 = np.nan_to_num(append_sp02,nan=0)\n",
    "append_sp02 = append_sp02.reshape(12664,12)\n",
    "\n",
    "append_rrate = scaler_rrate.transform(append_rrate)\n",
    "append_abpm = scaler_abpm.transform(append_abpm)\n",
    "append_sp02 = scaler_sp02.transform(append_sp02)\n",
    "\n",
    "test_df = np.concatenate((test_df, append_rrate, append_abpm, append_sp02), axis=1)"
   ]
  },
  {
   "source": [
    "using all meds is much better than just using one. however the norming doesnt work yet. it must be done for every single column. and scaling factors must be saved to be then applied to test data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = skl.linear_model.RidgeClassifierCV(alphas=[0.01, 0.01, 0.1, 1, 10, 100, 1000, 2000,5000], cv=15, normalize=True)\n",
    "clf = skl.svm.LinearSVC(dual=False, class_weight='balanced') #check out what balanced does\n",
    "#clf = skl.linear_model.LogisticRegressionCV(multi_class='multinomial')\n",
    "\n",
    "#lets do this for base excess and then see how it works\n",
    "train_data = df[Test].fillna(0).to_numpy()\n",
    "train_labels = labels[Test_labels].fillna(0).to_numpy()\n",
    "test_data = test_df_full[Test].fillna(0).to_numpy()\n",
    "predict_data = np.zeros((12664,10))\n",
    "\n",
    "#using localoutliers\n",
    "#lof = skl.neighbors.LocalOutlierFactor(n_neighbors= 100) #try out different factors\n",
    "#yhat = lof.fit_predict(train_data)\n",
    "\n",
    "#mask = yhat != -1\n",
    "#train_data, test_data = train_data[mask,:], train_labels[mask]\n",
    "#print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Task 1 finished with no Errors\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_labels[mask,:]\n",
    "\n",
    "for j in range(0,10):\n",
    "    clf.fit(fit_df,train_labels[:,j])\n",
    "    predict_data[:,j] = special.expit(clf.decision_function(test_df))\n",
    "    #print(predict_data)\n",
    "    #print(np.max(predict_data))\n",
    "\n",
    "df_task1 = pd.DataFrame(data=predict_data, columns= Test_labels)\n",
    "print(\"Task 1 finished with no Errors\")"
   ]
  },
  {
   "source": [
    "## TASK 2 #######################################################################################################"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Task 2 finished with no Errors\n"
     ]
    }
   ],
   "source": [
    "clf = skl.svm.LinearSVC(dual=False, class_weight='balanced')\n",
    "\n",
    "\n",
    "X = fit_df\n",
    "Y = labels_sebsis.ravel()       #.ravel() to circumvent a data conversion problem.\n",
    "\n",
    "clf.fit(X,Y)\n",
    "predict_sepsis = special.expit(clf.decision_function(test_df))\n",
    "\n",
    "df_task2 = pd.DataFrame({'LABEL_Sepsis': predict_sepsis})\n",
    "print(\"Task 2 finished with no Errors\")"
   ]
  },
  {
   "source": [
    "## TASK 3 #######################################################################################################\n",
    "\n",
    "lets try and remove some outliers from the dataset: Top Res: 8.679543614001695"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier removed\n",
    "\n",
    "data_mean, data_std = np.mean(fit_df), np.std(fit_df)\n",
    "\n",
    "#using isolationforest\n",
    "#iso = skl.ensemble.IsolationForest(contamination=0.1)\n",
    "#yhat = iso.fit_predict(fit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8.587728684107722\n",
      "Task 3 finished with no Errors\n"
     ]
    }
   ],
   "source": [
    "clf = skl.linear_model.RidgeCV(alphas=[0.01, 0.01, 0.1, 1, 10, 100, 1000, 2000,5000], cv=15)\n",
    "\n",
    "#try expanding the dataset with the measurements of the vitals and fit again\n",
    "\n",
    "#predict heart rate:\n",
    "clf.fit(fit_df,labels_hr)\n",
    "predict_train = clf.predict(fit_df)\n",
    "print(np.sqrt(np.mean((predict_train-labels_hr)**2)))\n",
    "predict_heartrate = clf.predict(test_df)\n",
    "\n",
    "#predict abpm:\n",
    "clf.fit(fit_df,labels_abpm)\n",
    "predict_abpm = clf.predict(test_df)\n",
    "\n",
    "#predict rrate:\n",
    "clf.fit(fit_df,labels_rrate)\n",
    "predict_rrate = clf.predict(test_df)\n",
    "\n",
    "#predict spo2:\n",
    "clf.fit(fit_df,labels_spo2)\n",
    "predict_sp02 = clf.predict(test_df)\n",
    "\n",
    "#For some reason linear regression works the best. but i still dont think it's acurate enough\n",
    "print(\"Task 3 finished with no Errors\")"
   ]
  },
  {
   "source": [
    "## TASK 4 #######################################################################################################\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method NDFrame.head of        LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate\n0        14.958466   83.515797   98.341426        91.640415\n1        17.604399   84.748636   95.009713       102.506903\n2        18.177836   80.935894   98.130527        89.576503\n3        17.237580   75.287592   95.811733        87.876922\n4        19.102455   72.965277   95.955606        60.145212\n...            ...         ...         ...              ...\n12659    20.219508   67.693750   95.642758       103.111121\n12660    18.672871   89.879290   98.552363        75.287247\n12661    19.370356   64.147715   96.720832        83.543792\n12662    16.572330   92.111848   97.913320        97.806558\n12663    17.852010   77.752273   98.466542        87.867207\n\n[12664 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "df_task3 = pd.DataFrame({'LABEL_RRate': predict_rrate[:,0], 'LABEL_ABPm': predict_abpm[:,0], 'LABEL_SpO2': predict_sp02[:,0],'LABEL_Heartrate': predict_heartrate[:,0]})\n",
    "print(df_task3.head)\n",
    "\n",
    "#print(fit_df)\n",
    "#print(labels-predict_train)\n",
    "#print(predict_train)\n",
    "#print(predict_heartrate)"
   ]
  },
  {
   "source": [
    "## Final Concatenate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frickin_df = pd.concat([pid_test ,df_task1, df_task2, df_task3], axis=1)\n",
    "final_frickin_df = final_frickin_df.round(8)\n",
    "compression_opts = dict(method='zip', archive_name='result.csv')\n",
    "final_frickin_df.to_csv('result.zip', index=False, compression=compression_opts)"
   ]
  }
 ]
}