{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sklearn.preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import special\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "np.random.seed(69) #fixing seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train_split = df_train['Sequence'].apply(lambda x: pd.Series(list(x))) #splits the 4 mutators into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test_split = df_test['Sequence'].apply(lambda x: pd.Series(list(x))) #splits the 4 mutators into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define universe of possible input values\n",
    "mutators = 'ACDEFGHIKLMNPQRSTUVWY' #abcdefghijklmnopqrstuvwxyz\n",
    "#bjoqxz <-- these letters are missing\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(mutators))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(mutators))\n",
    "\n",
    "# integer encode input data\n",
    "#integer_encoded = [char_to_int[item] for item in df_letters[0]]\n",
    "\n",
    "# Define the label for the NN which is the activation of the protein.\n",
    "labels = df_train['Active'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some constants.\n",
    "\n",
    "NUM_MUTATORS = len(mutators)\n",
    "\n",
    "DF_TRAIN_SIZE = len(df_train.index)\n",
    "DF_TEST_SIZE = len(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_onehot = np.zeros((DF_TRAIN_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "#We convert the mutators into the OneHot representation.\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TRAIN_SIZE):\n",
    "        temp = char_to_int[df_train_split.loc[j,i]]\n",
    "        df_train_onehot[j, i*NUM_MUTATORS + temp] = 1      #binary representation of the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same with the test dataframe\n",
    "df_test_onehot = np.zeros((DF_TEST_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TEST_SIZE):\n",
    "        temp = char_to_int[df_test_split.loc[j,i]]\n",
    "        df_test_onehot[j, i*NUM_MUTATORS + temp] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#lets norm this shit (this is only necessary if we use the 4D representation\n",
    "#df_nn = (df+1)/21\n",
    "#df_cock_nn = (df_cock+1)/21\n",
    "\n",
    "#for the binary version use this:\n",
    "df_train_onehot_nn = df_train_onehot\n",
    "df_test_onehot_nn = df_test_onehot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 21484), started 0:14:01 ago. (Use '!kill 21484' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some callbacks to stop overfitting\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when val_loss is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # Take the minimum of val_loss as a guideline.\n",
    "        mode=\"min\",\n",
    "        # No longer improving\" being further defined as \"for at least 50 epochs.\n",
    "        patience=50,\n",
    "        # Since we have such a large paitience we restore back to the best weights after a long waittime.\n",
    "        restore_best_weights = True,\n",
    "        verbose = 1,\n",
    "    )\n",
    "]\n",
    "\n",
    "## Some \n",
    "        #log_dir=log_dir,\n",
    "        #histogram_freq=1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10000\n",
      "66/66 - 2s - loss: 0.5189 - accuracy: 0.7940 - val_loss: 0.3090 - val_accuracy: 0.9622\n",
      "Epoch 2/10000\n",
      "66/66 - 1s - loss: 0.1703 - accuracy: 0.9797 - val_loss: 0.1704 - val_accuracy: 0.9622\n",
      "Epoch 3/10000\n",
      "66/66 - 1s - loss: 0.0612 - accuracy: 0.9890 - val_loss: 0.1744 - val_accuracy: 0.9622\n",
      "Epoch 4/10000\n",
      "66/66 - 1s - loss: 0.0327 - accuracy: 0.9935 - val_loss: 0.1820 - val_accuracy: 0.9622\n",
      "Epoch 5/10000\n",
      "66/66 - 1s - loss: 0.0223 - accuracy: 0.9951 - val_loss: 0.1618 - val_accuracy: 0.9631\n",
      "Epoch 6/10000\n",
      "66/66 - 1s - loss: 0.0153 - accuracy: 0.9967 - val_loss: 0.1281 - val_accuracy: 0.9679\n",
      "Epoch 7/10000\n",
      "66/66 - 1s - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.0926 - val_accuracy: 0.9751\n",
      "Epoch 8/10000\n",
      "66/66 - 1s - loss: 0.0082 - accuracy: 0.9984 - val_loss: 0.0639 - val_accuracy: 0.9819\n",
      "Epoch 9/10000\n",
      "66/66 - 1s - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0494 - val_accuracy: 0.9860\n",
      "Epoch 10/10000\n",
      "66/66 - 1s - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0428 - val_accuracy: 0.9878\n",
      "Epoch 11/10000\n",
      "66/66 - 1s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.0432 - val_accuracy: 0.9882\n",
      "Epoch 12/10000\n",
      "66/66 - 1s - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.0465 - val_accuracy: 0.9873\n",
      "Epoch 13/10000\n",
      "66/66 - 1s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0594 - val_accuracy: 0.9853\n",
      "Epoch 14/10000\n",
      "66/66 - 1s - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.0559 - val_accuracy: 0.9863\n",
      "Epoch 15/10000\n",
      "66/66 - 1s - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.0476 - val_accuracy: 0.9880\n",
      "Epoch 16/10000\n",
      "66/66 - 1s - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0454 - val_accuracy: 0.9886\n",
      "Epoch 17/10000\n",
      "66/66 - 1s - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0479 - val_accuracy: 0.9877\n",
      "Epoch 18/10000\n",
      "66/66 - 1s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0467 - val_accuracy: 0.9886\n",
      "Epoch 19/10000\n",
      "66/66 - 1s - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0457 - val_accuracy: 0.9885\n",
      "Epoch 20/10000\n",
      "66/66 - 1s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0463 - val_accuracy: 0.9889\n",
      "Epoch 21/10000\n",
      "66/66 - 1s - loss: 9.6970e-04 - accuracy: 0.9998 - val_loss: 0.0464 - val_accuracy: 0.9885\n",
      "Epoch 22/10000\n",
      "66/66 - 1s - loss: 8.3402e-04 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9891\n",
      "Epoch 23/10000\n",
      "66/66 - 1s - loss: 4.2109e-04 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9895\n",
      "Epoch 24/10000\n",
      "66/66 - 1s - loss: 7.1331e-04 - accuracy: 0.9999 - val_loss: 0.0476 - val_accuracy: 0.9889\n",
      "Epoch 25/10000\n",
      "66/66 - 1s - loss: 9.0815e-04 - accuracy: 0.9998 - val_loss: 0.0527 - val_accuracy: 0.9884\n",
      "Epoch 26/10000\n",
      "66/66 - 1s - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0591 - val_accuracy: 0.9876\n",
      "Epoch 27/10000\n",
      "66/66 - 1s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0604 - val_accuracy: 0.9870\n",
      "Epoch 28/10000\n",
      "66/66 - 1s - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9857\n",
      "Epoch 29/10000\n",
      "66/66 - 1s - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.0701 - val_accuracy: 0.9850\n",
      "Epoch 30/10000\n",
      "66/66 - 1s - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0515 - val_accuracy: 0.9877\n",
      "Epoch 31/10000\n",
      "66/66 - 1s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0498 - val_accuracy: 0.9884\n",
      "Epoch 32/10000\n",
      "66/66 - 1s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0466 - val_accuracy: 0.9890\n",
      "Epoch 33/10000\n",
      "66/66 - 1s - loss: 7.5302e-04 - accuracy: 0.9999 - val_loss: 0.0476 - val_accuracy: 0.9890\n",
      "Epoch 34/10000\n",
      "66/66 - 1s - loss: 4.4646e-04 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9892\n",
      "Epoch 35/10000\n",
      "66/66 - 1s - loss: 5.1507e-04 - accuracy: 0.9999 - val_loss: 0.0493 - val_accuracy: 0.9891\n",
      "Epoch 36/10000\n",
      "66/66 - 1s - loss: 3.4807e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9893\n",
      "Epoch 37/10000\n",
      "66/66 - 1s - loss: 1.9156e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9895\n",
      "Epoch 38/10000\n",
      "66/66 - 1s - loss: 1.5899e-04 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9893\n",
      "Epoch 39/10000\n",
      "66/66 - 1s - loss: 1.4648e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9895\n",
      "Epoch 40/10000\n",
      "66/66 - 1s - loss: 1.2712e-04 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9895\n",
      "Epoch 41/10000\n",
      "66/66 - 1s - loss: 1.1226e-04 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9894\n",
      "Epoch 42/10000\n",
      "66/66 - 1s - loss: 1.0822e-04 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9896\n",
      "Epoch 43/10000\n",
      "66/66 - 1s - loss: 9.6971e-05 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9896\n",
      "Epoch 44/10000\n",
      "66/66 - 1s - loss: 9.2919e-05 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9895\n",
      "Epoch 45/10000\n",
      "66/66 - 1s - loss: 8.8328e-05 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9895\n",
      "Epoch 46/10000\n",
      "66/66 - 1s - loss: 8.1222e-05 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9895\n",
      "Epoch 47/10000\n",
      "66/66 - 1s - loss: 7.2514e-05 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9895\n",
      "Epoch 48/10000\n",
      "66/66 - 1s - loss: 6.8037e-05 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9895\n",
      "Epoch 49/10000\n",
      "66/66 - 1s - loss: 6.6692e-05 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9896\n",
      "Epoch 50/10000\n",
      "66/66 - 1s - loss: 5.9643e-05 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9896\n",
      "Epoch 51/10000\n",
      "66/66 - 1s - loss: 5.9376e-05 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9897\n",
      "Epoch 52/10000\n",
      "66/66 - 1s - loss: 5.5229e-05 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9896\n",
      "Epoch 53/10000\n",
      "66/66 - 1s - loss: 5.5003e-05 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9895\n",
      "Epoch 54/10000\n",
      "66/66 - 1s - loss: 4.9534e-05 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9895\n",
      "Epoch 55/10000\n",
      "66/66 - 1s - loss: 4.6354e-05 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9896\n",
      "Epoch 56/10000\n",
      "66/66 - 1s - loss: 4.3304e-05 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9895\n",
      "Epoch 57/10000\n",
      "66/66 - 1s - loss: 4.4849e-05 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9895\n",
      "Epoch 58/10000\n",
      "66/66 - 1s - loss: 4.2133e-05 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9896\n",
      "Epoch 59/10000\n",
      "66/66 - 1s - loss: 3.6610e-05 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9895\n",
      "Epoch 60/10000\n",
      "66/66 - 1s - loss: 3.7949e-05 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9896\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00060: early stopping\n",
      "[[0.00132886]\n",
      " [0.00197142]\n",
      " [0.00060037]\n",
      " ...\n",
      " [0.00052276]\n",
      " [0.00221708]\n",
      " [0.2048899 ]]\n",
      "[[0.00058794]\n",
      " [0.00069997]\n",
      " [0.00067934]\n",
      " ...\n",
      " [0.00096616]\n",
      " [0.00642899]\n",
      " [0.00054595]]\n"
     ]
    }
   ],
   "source": [
    "#lets construct the neural net:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(128, activation='relu', input_dim=NUM_MUTATORS * 4))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(df_train_onehot_nn, labels, epochs=10000, batch_size=1024, verbose=2, callbacks= callbacks, validation_split = 0.4)\n",
    "\n",
    "#version without loss monitoring\n",
    "#neuralNetwork.fit(df_train_onehot_nn, labels, epochs=100, batch_size=1024, verbose=1, validation_split = 0.4)\n",
    "\n",
    "\n",
    "#evaluation of the network prediction\n",
    "predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "print(predict_test_nn)\n",
    "print(predict_train_nn)\n",
    "predict_test_nn = (predict_test_nn >= 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n2.6119238e-07\nactual labels: 4213\npredicted labels: 4101\nthe f1 score is: 0.9805147943228291\n"
     ]
    }
   ],
   "source": [
    "print(predict_train_nn.max())\n",
    "print(predict_train_nn.min())\n",
    "\n",
    "print(\"actual labels:\", np.sum(labels))\n",
    "print(\"predicted labels:\", np.sum(predict_train_nn >0.5))\n",
    "print(\"the f1 score is:\", f1_score(labels.astype('bool'), (predict_train_nn >=0.5).astype('bool')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluation of the NN on the train data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "predict_test_nn = pd.DataFrame(predict_test_nn.astype('int'))\n",
    "predict_test_nn.to_csv('predictions.csv', header = False, index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}