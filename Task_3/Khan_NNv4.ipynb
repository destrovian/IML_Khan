{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0035816c817341503006e0a8714912042b4c6673b969b2520b787454674f444b8",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import special\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "np.random.seed(69) #fixing seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_splitt = df_train['Sequence'].apply(lambda x: pd.Series(list(x))) #splitts the 4 mutators into individual columns.\n",
    "df_active = df_train['Active']\n",
    "#df_letters = pd.concat([df_train['Sequence'].apply(lambda x: pd.Series(list(x))), df_train['Active']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test_s = df_test['Sequence'].apply(lambda x: pd.Series(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define universe of possible input values\n",
    "mutators = 'ACDEFGHIKLMNPQRSTVWY' #abcdefghijklmnopqrstuvwxyz\n",
    "#bjoquxz <-- these letters are missing\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(mutators))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(mutators))\n",
    "\n",
    "# integer encode input data\n",
    "#integer_encoded = [char_to_int[item] for item in df_letters[0]]\n",
    "#print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some constants.\n",
    "\n",
    "NUM_MUTATORS = len(mutators)\n",
    "\n",
    "DF_TRAIN_SIZE = len(df_train.index)\n",
    "DF_TEST_SIZE = len(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = np.zeros((DF_TRAIN_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "df_occurency = np.zeros((NUM_MUTATORS*4, 1))\n",
    "#Possibly the worst fucking way we could have done this. But fuckem.\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TRAIN_SIZE):\n",
    "        temp = char_to_int[df_splitt.loc[j,i]]\n",
    "        df[j, i * NUM_MUTATORS + temp] = 1      #binary representation of the letters\n",
    "        #df[j, i] = temp             #letter as float (0,1]\n",
    "        if df_active[j] == 1:\n",
    "            #Did the mutator at that spot actually lead to a activation? \n",
    "            df_occurency[i* NUM_MUTATORS + temp] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same with the test dataframe\n",
    "\n",
    "df_cock = np.zeros((DF_TEST_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TEST_SIZE):\n",
    "        temp = char_to_int[df_test_s.loc[j,i]]\n",
    "        df_cock[j, i * NUM_MUTATORS + temp] = 1\n",
    "        #df_cock[j, i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of mutators at that spot\n",
    "df_sum = df.sum(axis=0)\n",
    "df_probability = np.zeros((NUM_MUTATORS*4, 1))\n",
    "for i in range(0,NUM_MUTATORS * 4):\n",
    "    df_probability[i] = df_occurency[i]/df_sum[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prob = np.zeros((DF_TRAIN_SIZE, NUM_MUTATORS*4))\n",
    "df_test_prob = np.zeros((DF_TEST_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "#Possibly the worst fucking way we could have done this. But fuckem.\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TRAIN_SIZE):\n",
    "        temp = char_to_int[df_splitt.loc[j,i]]\n",
    "        df_train_prob[j, i * NUM_MUTATORS + temp] = df_probability[i* NUM_MUTATORS + temp]      #binary representation of the letters converted to probability of activation\n",
    "        #df[j, i] = temp             #letter as float (0,1]\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TEST_SIZE):\n",
    "        temp = char_to_int[df_test_s.loc[j,i]]\n",
    "        df_test_prob[j, i * NUM_MUTATORS + temp] = df_probability[i* NUM_MUTATORS + temp]\n",
    "        #df_cock[j, i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done with fitting\n"
     ]
    }
   ],
   "source": [
    "#clf = skl.linear_model.RidgeCV(alphas=[0.01, 0.01, 0.1, 1, 10, 100, 1000, 2000,5000], cv=15)\n",
    "\n",
    "labels = df_train['Active'].to_numpy()\n",
    "\n",
    "#predict cocks:\n",
    "#clf.fit(df,labels)\n",
    "print('Done with fitting')\n",
    "#predict_train = clf.predict(df)\n",
    "#print('Done with prediction and error is: ', np.sqrt(np.mean((predict_train-labels)**2)))\n",
    "#predict_cancer = clf.predict(df_cock)\n",
    "#predict_cancer = np.where(predict_cancer > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with fitting\n"
     ]
    }
   ],
   "source": [
    "#clf = skl.svm.LinearSVC(dual=False, class_weight='balanced')\n",
    "\n",
    "#labels = df_train['Active'].to_numpy()\n",
    "\n",
    "#predict cocks:\n",
    "#clf.fit(df,labels)\n",
    "print('Done with fitting')\n",
    "#predict_train = clf.predict(df)\n",
    "#print('Done with prediction and error is: ', np.sqrt(np.mean((predict_train-labels)**2)))\n",
    "#predict_cancer = clf.predict(df_cock)\n",
    "#predict_cancer = np.where(predict_cancer > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#np.savetxt('submission_sklearn.csv', predict_cancer, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "#lets norm this shit (this is only necessary if we use the 4D representation\n",
    "#df_nn = (df+1)/21\n",
    "#df_cock_nn = (df_cock+1)/21\n",
    "\n",
    "#for the binary version use this:\n",
    "df_nn = df\n",
    "df_cock_nn = df_cock\n",
    "\n",
    "#print(df_nn.max(), df_nn.min())\n",
    "#print(labels.max(), labels.min())\n",
    "#print(df_cock_nn.max(), df_cock_nn.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3.1055799e-06]\n [2.1041219e-06]\n [2.2878929e-07]\n ...\n [1.3161004e-03]\n [1.3093959e-05]\n [7.8472793e-03]]\n[[3.8820676e-06]\n [1.0311024e-05]\n [1.0347408e-06]\n ...\n [1.3235198e-05]\n [1.8877844e-07]\n [1.1063612e-05]]\n"
     ]
    }
   ],
   "source": [
    "#lets construct the neural net:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(128, activation='relu', input_dim=80))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(df_nn, labels, epochs=50, batch_size=1024, verbose=0)  #<-- OLD\n",
    "\n",
    "#evaluation of the network prediction\n",
    "predict_cancer_nn = neuralNetwork.predict(df_cock_nn)\n",
    "predict_train_nn = neuralNetwork.predict(df_nn)\n",
    "print(predict_cancer_nn)\n",
    "print(predict_train_nn)\n",
    "predict_cancer_nn = (predict_cancer_nn >= 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "110/110 - 1s - loss: 0.0162 - accuracy: 0.9939\n",
      "Epoch 2/50\n",
      "110/110 - 1s - loss: 0.0163 - accuracy: 0.9938\n",
      "Epoch 3/50\n",
      "110/110 - 1s - loss: 0.0162 - accuracy: 0.9939\n",
      "Epoch 4/50\n",
      "110/110 - 1s - loss: 0.0134 - accuracy: 0.9949\n",
      "Epoch 5/50\n",
      "110/110 - 1s - loss: 0.0136 - accuracy: 0.9949\n",
      "Epoch 6/50\n",
      "110/110 - 1s - loss: 0.0125 - accuracy: 0.9951\n",
      "Epoch 7/50\n",
      "110/110 - 1s - loss: 0.0125 - accuracy: 0.9952\n",
      "Epoch 8/50\n",
      "110/110 - 1s - loss: 0.0127 - accuracy: 0.9950\n",
      "Epoch 9/50\n",
      "110/110 - 1s - loss: 0.0114 - accuracy: 0.9956\n",
      "Epoch 10/50\n",
      "110/110 - 1s - loss: 0.0116 - accuracy: 0.9953\n",
      "Epoch 11/50\n",
      "110/110 - 1s - loss: 0.0108 - accuracy: 0.9957\n",
      "Epoch 12/50\n",
      "110/110 - 1s - loss: 0.0099 - accuracy: 0.9964\n",
      "Epoch 13/50\n",
      "110/110 - 1s - loss: 0.0094 - accuracy: 0.9964\n",
      "Epoch 14/50\n",
      "110/110 - 1s - loss: 0.0097 - accuracy: 0.9963\n",
      "Epoch 15/50\n",
      "110/110 - 1s - loss: 0.0100 - accuracy: 0.9961\n",
      "Epoch 16/50\n",
      "110/110 - 1s - loss: 0.0094 - accuracy: 0.9963\n",
      "Epoch 17/50\n",
      "110/110 - 1s - loss: 0.0088 - accuracy: 0.9967\n",
      "Epoch 18/50\n",
      "110/110 - 1s - loss: 0.0091 - accuracy: 0.9966\n",
      "Epoch 19/50\n",
      "110/110 - 1s - loss: 0.0093 - accuracy: 0.9965\n",
      "Epoch 20/50\n",
      "110/110 - 1s - loss: 0.0082 - accuracy: 0.9969\n",
      "Epoch 21/50\n",
      "110/110 - 1s - loss: 0.0078 - accuracy: 0.9971\n",
      "Epoch 22/50\n",
      "110/110 - 1s - loss: 0.0083 - accuracy: 0.9969\n",
      "Epoch 23/50\n",
      "110/110 - 1s - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 24/50\n",
      "110/110 - 1s - loss: 0.0077 - accuracy: 0.9971\n",
      "Epoch 25/50\n",
      "110/110 - 1s - loss: 0.0071 - accuracy: 0.9973\n",
      "Epoch 26/50\n",
      "110/110 - 1s - loss: 0.0062 - accuracy: 0.9977\n",
      "Epoch 27/50\n",
      "110/110 - 1s - loss: 0.0068 - accuracy: 0.9973\n",
      "Epoch 28/50\n",
      "110/110 - 1s - loss: 0.0069 - accuracy: 0.9974\n",
      "Epoch 29/50\n",
      "110/110 - 1s - loss: 0.0060 - accuracy: 0.9976\n",
      "Epoch 30/50\n",
      "110/110 - 1s - loss: 0.0076 - accuracy: 0.9971\n",
      "Epoch 31/50\n",
      "110/110 - 1s - loss: 0.0062 - accuracy: 0.9976\n",
      "Epoch 32/50\n",
      "110/110 - 1s - loss: 0.0062 - accuracy: 0.9977\n",
      "Epoch 33/50\n",
      "110/110 - 1s - loss: 0.0055 - accuracy: 0.9980\n",
      "Epoch 34/50\n",
      "110/110 - 1s - loss: 0.0071 - accuracy: 0.9974\n",
      "Epoch 35/50\n",
      "110/110 - 1s - loss: 0.0057 - accuracy: 0.9979\n",
      "Epoch 36/50\n",
      "110/110 - 1s - loss: 0.0039 - accuracy: 0.9984\n",
      "Epoch 37/50\n",
      "110/110 - 1s - loss: 0.0048 - accuracy: 0.9981\n",
      "Epoch 38/50\n",
      "110/110 - 1s - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 39/50\n",
      "110/110 - 1s - loss: 0.0054 - accuracy: 0.9979\n",
      "Epoch 40/50\n",
      "110/110 - 1s - loss: 0.0067 - accuracy: 0.9975\n",
      "Epoch 41/50\n",
      "110/110 - 1s - loss: 0.0057 - accuracy: 0.9979\n",
      "Epoch 42/50\n",
      "110/110 - 1s - loss: 0.0045 - accuracy: 0.9983\n",
      "Epoch 43/50\n",
      "110/110 - 1s - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "110/110 - 1s - loss: 0.0046 - accuracy: 0.9983\n",
      "Epoch 45/50\n",
      "110/110 - 1s - loss: 0.0044 - accuracy: 0.9984\n",
      "Epoch 46/50\n",
      "110/110 - 1s - loss: 0.0042 - accuracy: 0.9985\n",
      "Epoch 47/50\n",
      "110/110 - 1s - loss: 0.0039 - accuracy: 0.9985\n",
      "Epoch 48/50\n",
      "110/110 - 1s - loss: 0.0039 - accuracy: 0.9986\n",
      "Epoch 49/50\n",
      "110/110 - 1s - loss: 0.0039 - accuracy: 0.9986\n",
      "Epoch 50/50\n",
      "110/110 - 1s - loss: 0.0058 - accuracy: 0.9980\n",
      "[[2.3295547e-06]\n",
      " [2.6865303e-08]\n",
      " [1.0497442e-06]\n",
      " ...\n",
      " [7.8138709e-04]\n",
      " [7.2948666e-08]\n",
      " [6.9692505e-05]]\n",
      "[[1.9293670e-06]\n",
      " [4.7935032e-06]\n",
      " [4.4423587e-07]\n",
      " ...\n",
      " [1.9870259e-09]\n",
      " [9.1534294e-11]\n",
      " [2.9957707e-09]]\n"
     ]
    }
   ],
   "source": [
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(df_train_prob, labels, epochs=50, batch_size=1024, verbose=2)  #<-- NEW\n",
    "\n",
    "#evaluation of the network prediction\n",
    "predict_testp_nn = neuralNetwork.predict(df_test_prob)\n",
    "predict_trainp_nn = neuralNetwork.predict(df_train_prob)\n",
    "print(predict_testp_nn)\n",
    "print(predict_trainp_nn)\n",
    "predict_testp_nn = (predict_testp_nn >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.3374322e-13\n",
      "actual labels: 4213\n",
      "predicted labels: 4224\n",
      "the f1 score is: 0.9979850657816759\n"
     ]
    }
   ],
   "source": [
    "print(predict_train_nn.max())\n",
    "print(predict_train_nn.min())\n",
    "\n",
    "print(\"actual labels:\", np.sum(labels))\n",
    "print(\"predicted labels:\", np.sum(predict_train_nn >0.5))\n",
    "print(\"the f1 score is:\", f1_score(labels.astype('bool'), (predict_train_nn >=0.5).astype('bool')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluation of the NN on the train data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "predict_cancer_nn = pd.DataFrame(predict_cancer_nn.astype('int'))\n",
    "predict_cancer_nn.to_csv('predictions.csv', header = False, index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n1.1738408e-17\nactual labels: 4213\npredicted labels: 4029\nthe f1 score is: 0.9728221305508372\n"
     ]
    }
   ],
   "source": [
    "#New\n",
    "print(predict_trainp_nn.max())\n",
    "print(predict_trainp_nn.min())\n",
    "\n",
    "print(\"actual labels:\", np.sum(labels))\n",
    "print(\"predicted labels:\", np.sum(predict_trainp_nn >0.5))\n",
    "print(\"the f1 score is:\", f1_score(labels.astype('bool'), (predict_trainp_nn >=0.5).astype('bool')))\n",
    "\n",
    "predict_testp_nn = pd.DataFrame(predict_testp_nn.astype('int'))\n",
    "predict_testp_nn.to_csv('predictions_with_probability.csv', header = False, index = False)\n"
   ]
  }
 ]
}