{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import special\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "np.random.seed(69) #fixing seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train_split = df_train['Sequence'].apply(lambda x: pd.Series(list(x))) #splits the 4 mutators into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test_split = df_test['Sequence'].apply(lambda x: pd.Series(list(x))) #splits the 4 mutators into individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define universe of possible input values\n",
    "mutators = 'ACDEFGHIKLMNPQRSTUVWY' #abcdefghijklmnopqrstuvwxyz\n",
    "#bjoqxz <-- these letters are missing\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(mutators))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(mutators))\n",
    "\n",
    "# integer encode input data\n",
    "#integer_encoded = [char_to_int[item] for item in df_letters[0]]\n",
    "\n",
    "# Define the label for the NN which is the activation of the protein.\n",
    "labels = df_train['Active'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some constants.\n",
    "\n",
    "NUM_MUTATORS = len(mutators)\n",
    "\n",
    "DF_TRAIN_SIZE = len(df_train.index)\n",
    "DF_TEST_SIZE = len(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_onehot = np.zeros((DF_TRAIN_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "#We convert the mutators into the OneHot representation.\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TRAIN_SIZE):\n",
    "        temp = char_to_int[df_train_split.loc[j,i]]\n",
    "        df_train_onehot[j, i*NUM_MUTATORS + temp] = 1      #binary representation of the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same with the test dataframe\n",
    "df_test_onehot = np.zeros((DF_TEST_SIZE, NUM_MUTATORS*4))\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0, DF_TEST_SIZE):\n",
    "        temp = char_to_int[df_test_split.loc[j,i]]\n",
    "        df_test_onehot[j, i*NUM_MUTATORS + temp] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#lets norm this shit (this is only necessary if we use the 4D representation\n",
    "#df_nn = (df+1)/21\n",
    "#df_cock_nn = (df_cock+1)/21\n",
    "\n",
    "#for the binary version use this:\n",
    "df_train_onehot_nn = df_train_onehot\n",
    "df_test_onehot_nn = df_test_onehot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some callbacks to stop overfitting\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when val_loss is no longer improving\n",
    "        monitor=\"loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-3,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 2s 12ms/step - loss: 0.5723 - accuracy: 0.7231\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.0962 - accuracy: 0.9843\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.0372 - accuracy: 0.9903\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.0224 - accuracy: 0.9934\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.0172 - accuracy: 0.9949\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.0119 - accuracy: 0.9963\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.0097 - accuracy: 0.9971\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 1s 11ms/step - loss: 0.0078 - accuracy: 0.9979\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "[[4.5768619e-03]\n",
      " [2.5471716e-05]\n",
      " [1.2740493e-03]\n",
      " ...\n",
      " [3.1971931e-04]\n",
      " [2.0465255e-04]\n",
      " [3.6787689e-03]]\n",
      "[[3.6579370e-04]\n",
      " [6.2578917e-04]\n",
      " [7.1820617e-04]\n",
      " ...\n",
      " [8.7097287e-04]\n",
      " [5.1130964e-06]\n",
      " [3.5962462e-04]]\n"
     ]
    }
   ],
   "source": [
    "#lets construct the neural net:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(128, activation='relu', input_dim=NUM_MUTATORS * 4))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(128, activation='relu'))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "#neuralNetwork.fit(df_train_onehot_nn, labels, epochs=50, batch_size=1024, verbose=1, callbacks= callbacks)\n",
    "\n",
    "#version without loss monitoring\n",
    "neuralNetwork.fit(df_train_onehot_nn, labels, epochs=10, batch_size=1024, verbose=1)\n",
    "\n",
    "\n",
    "#evaluation of the network prediction\n",
    "predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "print(predict_test_nn)\n",
    "print(predict_train_nn)\n",
    "predict_test_nn = (predict_test_nn >= 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n2.6119238e-07\nactual labels: 4213\npredicted labels: 4101\nthe f1 score is: 0.9805147943228291\n"
     ]
    }
   ],
   "source": [
    "print(predict_train_nn.max())\n",
    "print(predict_train_nn.min())\n",
    "\n",
    "print(\"actual labels:\", np.sum(labels))\n",
    "print(\"predicted labels:\", np.sum(predict_train_nn >0.5))\n",
    "print(\"the f1 score is:\", f1_score(labels.astype('bool'), (predict_train_nn >=0.5).astype('bool')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluation of the NN on the train data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "predict_test_nn = pd.DataFrame(predict_test_nn.astype('int'))\n",
    "predict_test_nn.to_csv('predictions.csv', header = False, index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}