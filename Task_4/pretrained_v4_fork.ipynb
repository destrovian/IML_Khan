{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import layers\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import class_weight\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = VGG16(include_top=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_features(ID):\n",
    "    img_path = 'food_porn/food/' + ID + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = np.array(model.predict(x))\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PIL.Image.Image image mode=RGB size=224x224 at 0x1ED81D0E520>\n____________________\n[[[200. 200. 236.]\n  [194. 194. 230.]\n  [197. 201. 236.]\n  ...\n  [156. 169. 113.]\n  [159. 178. 123.]\n  [149. 173. 123.]]\n\n [[201. 201. 237.]\n  [193. 193. 229.]\n  [199. 203. 238.]\n  ...\n  [157. 175. 117.]\n  [164. 185. 129.]\n  [137. 164. 113.]]\n\n [[191. 191. 227.]\n  [188. 191. 226.]\n  [187. 191. 226.]\n  ...\n  [139. 167. 106.]\n  [150. 179. 123.]\n  [141. 174. 121.]]\n\n ...\n\n [[165. 191. 128.]\n  [127. 156.  92.]\n  [130. 159.  95.]\n  ...\n  [189. 222. 253.]\n  [176. 205. 237.]\n  [186. 203. 229.]]\n\n [[145. 177. 112.]\n  [140. 175. 109.]\n  [120. 155.  89.]\n  ...\n  [187. 222. 250.]\n  [176. 206. 230.]\n  [180. 197. 207.]]\n\n [[140. 177. 110.]\n  [149. 186. 119.]\n  [147. 184. 117.]\n  ...\n  [187. 223. 249.]\n  [180. 209. 225.]\n  [177. 193. 193.]]]\n"
     ]
    }
   ],
   "source": [
    "image_1 = image.load_img('food_porn/food/' + '00050' + '.jpg', target_size=(224, 224))\n",
    "image_s = img_to_array(image_1)\n",
    "print(image_1)\n",
    "print('____________________')\n",
    "print(image_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 0us/step\n",
      "[[('n04476259', 'tray', 0.27396187), ('n03485794', 'handkerchief', 0.23914962), ('n03291819', 'envelope', 0.18719405), ('n03871628', 'packet', 0.06075603), ('n03961711', 'plate_rack', 0.026308354)]]\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(decode_predictions(get_features('00000')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n",
      "801\n",
      "901\n",
      "1001\n",
      "1101\n",
      "1201\n",
      "1301\n",
      "1401\n",
      "1501\n",
      "1601\n",
      "1701\n",
      "1801\n",
      "1901\n",
      "2001\n",
      "2101\n",
      "2201\n",
      "2301\n",
      "2401\n",
      "2501\n",
      "2601\n",
      "2701\n",
      "2801\n",
      "2901\n",
      "3001\n",
      "3101\n",
      "3201\n",
      "3301\n",
      "3401\n",
      "3501\n",
      "3601\n",
      "3701\n",
      "3801\n",
      "3901\n",
      "4001\n",
      "4101\n",
      "4201\n",
      "4301\n",
      "4401\n",
      "4501\n",
      "4601\n",
      "4701\n",
      "4801\n",
      "4901\n",
      "5001\n",
      "5101\n",
      "5201\n",
      "5301\n",
      "5401\n",
      "5501\n",
      "5601\n",
      "5701\n",
      "5801\n",
      "5901\n",
      "6001\n",
      "6101\n",
      "6201\n",
      "6301\n",
      "6401\n",
      "6501\n",
      "6601\n",
      "6701\n",
      "6801\n",
      "6901\n",
      "7001\n",
      "7101\n",
      "7201\n",
      "7301\n",
      "7401\n",
      "7501\n",
      "7601\n",
      "7701\n",
      "7801\n",
      "7901\n",
      "8001\n",
      "8101\n",
      "8201\n",
      "8301\n",
      "8401\n",
      "8501\n",
      "8601\n",
      "8701\n",
      "8801\n",
      "8901\n",
      "9001\n",
      "9101\n",
      "9201\n",
      "9301\n",
      "9401\n",
      "9501\n",
      "9601\n",
      "9701\n",
      "9801\n",
      "9901\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.zeros((10000,1001))\n",
    "id = np.array(range(0,10000))\n",
    "data[:,0] = id.transpose()\n",
    "\n",
    "def features():\n",
    "    for i in range(0,10000):\n",
    "        featureid = '{0:05}'.format(i)\n",
    "        features = get_features(featureid)\n",
    "        data[i,1:] = features\n",
    "        if i % 100 == True:\n",
    "            print(i)\n",
    "    return data\n",
    "\n",
    "data = features()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets read out all the features of the dataset:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ass = 'features_vgg16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10009999\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(data))\n",
    "save_ass = 'features_vgg16.csv'\n",
    "\n",
    "np.savetxt(save_ass, data, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using false we got:     527 correct\n",
    "using true we got:      520 correct\n",
    "\n",
    "how about... we use the true version to feed a NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int') \n",
    "\n",
    "data = np.genfromtxt(save_ass, delimiter= ',')\n",
    "data = np.array(data[:,1:])\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets prepare the data for the NN\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 0 100000\n",
      "we're at entry: 0 150000\n",
      "we're at entry: 0 200000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 1 100000\n",
      "we're at entry: 1 150000\n",
      "we're at entry: 1 200000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 50000\n",
      "we're at entry: 2 100000\n",
      "we're at entry: 2 150000\n",
      "we're at entry: 2 200000\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# There are 4 different permutations of ABC that we can know the answer of. (CAB and CBA we don't know)\n",
    "ONE_SET_SIZE = 59515\n",
    "\n",
    "#create the base dataset with the 2 sets of labels\n",
    "data_y1 = np.ones((ONE_SET_SIZE * 2,1))\n",
    "data_y0 = np.zeros((ONE_SET_SIZE * 2,1))\n",
    "\n",
    "#create the inverse dataset and a set where B is connected to A (and inv)\n",
    "\n",
    "ACB0 = np.zeros((ONE_SET_SIZE,3)).astype('int')    # One needs 3 different assignments since the arrays would be linked otherwise.\n",
    "BAC1 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "BCA0 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "\n",
    "ACB0[:,0] = ABC1[:,0]\n",
    "BAC1[:,1] = ABC1[:,0]\n",
    "BCA0[:,2] = ABC1[:,0]\n",
    "\n",
    "ACB0[:,2] = ABC1[:,1]\n",
    "BAC1[:,0] = ABC1[:,1]\n",
    "BCA0[:,0] = ABC1[:,1]\n",
    "\n",
    "ACB0[:,1] = ABC1[:,2]\n",
    "BAC1[:,2] = ABC1[:,2]\n",
    "BCA0[:,1] = ABC1[:,2]\n",
    "\n",
    "#assemble everything into one huge pile of S*** in order to shuffle it\n",
    "id_fin = np.concatenate((ABC1, BAC1, ACB0, BCA0),axis=0)\n",
    "labels = np.concatenate((data_y1,data_y0),axis=0)\n",
    "id_labels = np.concatenate((id_fin,labels),axis=1)\n",
    "\n",
    "data_nn_x = np.zeros((ONE_SET_SIZE * 4,3000))\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,ONE_SET_SIZE * 4):\n",
    "            data_nn_x[j,range(i*1000,1000*(i+1))] = load_features(id_labels[j,i].astype('int'))\n",
    "            if j%50000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "\n",
    "#print(train_trip)\n",
    "#print(train_trip_inv)\n",
    "#now we try to randomize the data in order to increase our accuracy\n",
    "# we may even double the dataset by using 0 as well as the 1 case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collab = np.concatenate((data_nn_x,labels), axis=1)\n",
    "np.random.shuffle(collab)\n",
    "\n",
    "data_x = collab[:,:3000]\n",
    "data_y = collab[:,3000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets shuffle the training data:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(\"%d bytes\" % (data_nn_x.size * data_nn_x.itemsize))\n",
    "#np.savetxt(\"nn_data.csv\",data_nn_x,delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers.Dense(\n",
    "    units=64,\n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-2, l2=1e-2),\n",
    "    bias_regularizer=regularizers.l2(1e-2),\n",
    "    activity_regularizer=regularizers.l2(1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#TAKE IT Branda:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(1024, activation='relu', input_dim=3000, kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.5))\n",
    "neuralNetwork.add(Dense(512, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.4))\n",
    "neuralNetwork.add(Dense(128, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(128, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(64, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.2))\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(data_x, data_y, epochs=128, batch_size=1024, verbose=1, validation_split = 0.2)\n",
    "\n",
    "#evaluation of the network prediction\n",
    "#predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "#predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "#print(predict_test_nn)\n",
    "#print(predict_train_nn)\n",
    "#predict_test_nn = (predict_test_nn >= 0.5)\n",
    "neuralNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 3360), started 3 days, 0:14:59 ago. (Use '!kill 3360' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 10000\n",
      "we're at entry: 0 20000\n",
      "we're at entry: 0 30000\n",
      "we're at entry: 0 40000\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 10000\n",
      "we're at entry: 1 20000\n",
      "we're at entry: 1 30000\n",
      "we're at entry: 1 40000\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 10000\n",
      "we're at entry: 2 20000\n",
      "we're at entry: 2 30000\n",
      "we're at entry: 2 40000\n",
      "we're at entry: 2 50000\n"
     ]
    }
   ],
   "source": [
    "test_trip = np.loadtxt(\"test_triplets.txt\").astype('int')\n",
    "test_nn_x = np.zeros((59544,3000))\n",
    "for i in range(0,3):\n",
    "        for j in range(0,59544):\n",
    "            test_nn_x[j,range(i*1000,1000*(i+1))] = load_features(test_trip[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "predict_test_nn = neuralNetwork.predict(test_nn_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% gotta try some shit:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "export = np.array(predict_test_nn>0.5).astype('int')\n",
    "np.savetxt(\"submission_vgg16_regularizer.txt\",np.round(export,decimals=0), fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% export prediction:\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0035816c817341503006e0a8714912042b4c6673b969b2520b787454674f444b8",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}