{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.utils import class_weight\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape: (None, 4096)\n"
     ]
    }
   ],
   "source": [
    "#model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = VGG16(include_top=True)\n",
    "\n",
    "last_layer = model.get_layer('fc1')\n",
    "print('last layer output shape:', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "\n",
    "FEATURE_SIZE = 2048\n",
    "reduction = 2048"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer._name = layer._name + str('_C')\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "cut_model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def get_features(ID):\n",
    "    img_path = 'food_porn/food/' + ID + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = np.array(cut_model.predict(x))\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 495 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#print(decode_predictions(get_features('00000')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.zeros((10000,4096 + 1))\n",
    "id = np.array(range(0,10000))\n",
    "data[:,0] = id.transpose()\n",
    "\n",
    "def features():\n",
    "    for i in range(0,10000):\n",
    "        featureid = '{0:05}'.format(i)\n",
    "        features = get_features(featureid)\n",
    "        data[i,1:] = features\n",
    "        if i % 100 == True:\n",
    "            print(i)\n",
    "    return data\n",
    "\n",
    "#data = features()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets read out all the features of the dataset:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_ass = 'features_vgg16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(data))\n",
    "save_ass = 'features_vgg16.csv'\n",
    "\n",
    "#np.savetxt(save_ass, data, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using false we got:     527 correct\n",
    "using true we got:      520 correct\n",
    "\n",
    "how about... we use the true version to feed a NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#lets prepare the data for the NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int') \n",
    "\n",
    "data = np.genfromtxt(save_ass, delimiter= ',')\n",
    "\n",
    "svd = PCA(n_components=reduction)\n",
    "svd.fit(data)\n",
    "transformed = svd.transform(data)\n",
    "print(transformed.shape)\n",
    "np.savetxt(\"transformed_2048_pca_vgg16.csv\", transformed, delimiter=',')\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2048)\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int')\n",
    "\n",
    "data = np.genfromtxt(\"transformed_2k_vgg16.csv\", delimiter= ',')\n",
    "data = np.array(data)\n",
    "#data = np.array(data[:,1:])\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data1 = np.genfromtxt(\"transformed_2k_vgg16.csv\", delimiter= ',')\n",
    "data2 = np.genfromtxt(\"transformed_2048_pca_vgg16.csv\", delimiter= ',')\n",
    "data3 = np.genfromtxt(\"transformed_2048_vgg16.csv\", delimiter= ',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(np.max(data1))\n",
    "print(np.min(data1))\n",
    "print(np.count_nonzero(data1>0)/10000)\n",
    "print(data1)\n",
    "\n",
    "print(np.max(data2))\n",
    "print(np.min(data2))\n",
    "print(np.count_nonzero(data2>0)/10000)\n",
    "print(data2)\n",
    "\n",
    "print(np.max(data3))\n",
    "print(np.min(data3))\n",
    "print(np.count_nonzero(data3>0)/10000)\n",
    "print(data3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437.1729314338028\n",
      "-232.46749067398565\n",
      "1023.6355\n",
      "[[ 1.13651419e+02 -7.90404109e+00  1.45638476e+01 ...  2.45957482e-02\n",
      "  -3.80949972e+00  1.88267921e+00]\n",
      " [ 1.63260723e+02 -2.88156321e+01  4.03460357e+00 ...  1.59725514e+00\n",
      "   8.89895754e-01 -1.33387738e-01]\n",
      " [ 1.20817501e+02  4.81291883e+01  3.80443678e+01 ... -2.35474535e+00\n",
      "  -5.82574743e-01  1.57955269e+00]\n",
      " ...\n",
      " [ 1.54377886e+02 -4.68704584e+00 -2.45622575e+00 ...  8.00527003e-01\n",
      "   9.71803475e-01  1.89183454e+00]\n",
      " [ 1.14112921e+02 -2.61596385e+01  7.62224940e+01 ...  1.08829451e+00\n",
      "  -4.25203554e-01 -2.99227767e+00]\n",
      " [ 1.15845917e+02  7.02213942e+01  5.27495035e+01 ... -7.76336616e-01\n",
      "   1.78615267e+00  2.14172536e+00]]\n",
      "4999.509837181176\n",
      "-4999.495408342369\n",
      "1023.455\n",
      "[[ 4.99950984e+03 -1.06269086e+01 -5.11564339e+01 ...  7.74110307e-01\n",
      "  -6.25773662e-01 -4.44572817e+00]\n",
      " [ 4.99849075e+03  3.11837707e+01 -2.76554560e+00 ...  2.11490466e+00\n",
      "  -1.26303460e-01 -9.46173446e-01]\n",
      " [ 4.99748581e+03 -6.21577864e+01 -4.08563130e+01 ...  6.70130156e-02\n",
      "  -2.28361882e-01 -1.71833483e+00]\n",
      " ...\n",
      " [-4.99749540e+03  9.98548153e+00  5.29981833e+00 ...  2.40180430e+00\n",
      "   6.82002511e-01  1.67461366e+00]\n",
      " [-4.99849144e+03  8.37073221e+00 -7.33605761e+01 ...  1.27637278e+00\n",
      "   1.03874594e-01 -9.87585062e-01]\n",
      " [-4.99949541e+03 -7.77342560e+01 -2.69338250e+01 ... -2.56707591e-01\n",
      "   3.81493263e-01 -2.65084422e+00]]\n",
      "9999.029115587078\n",
      "-213.15854894770533\n",
      "1023.9677\n",
      "[[ 2.66600400e+00  1.09232763e+02 -2.23940587e+00 ... -3.08169769e+00\n",
      "   1.20065402e+00 -3.73720656e+00]\n",
      " [ 4.76817876e+00  1.64029883e+02 -1.89724702e+01 ... -2.07202160e+00\n",
      "   1.84467302e+00  1.14869040e+00]\n",
      " [ 4.84987212e+00  1.12842655e+02  5.38047915e+01 ... -6.79186360e-01\n",
      "  -1.80310502e+00 -8.92419932e-01]\n",
      " ...\n",
      " [ 9.99787782e+03 -7.43207986e+01 -1.27782015e+01 ... -1.67272383e-01\n",
      "  -1.71778659e-01  7.74200211e-01]\n",
      " [ 9.99798523e+03 -1.18542587e+02 -3.83995737e+01 ... -1.71143864e+00\n",
      "   1.43298853e+00 -1.07249323e-01]\n",
      " [ 9.99902912e+03 -1.19284109e+02  5.87436913e+01 ...  1.75720157e+00\n",
      "  -1.80559322e+00  1.44730863e+00]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# There are 4 different permutations of ABC that we can know the answer of. (CAB and CBA we don't know)\n",
    "ONE_SET_SIZE = 59515\n",
    "\n",
    "#create the base dataset with the 2 sets of labels\n",
    "data_y1 = np.ones((ONE_SET_SIZE * 2,1))\n",
    "data_y0 = np.zeros((ONE_SET_SIZE * 2,1))\n",
    "\n",
    "#create the inverse dataset and a set where B is connected to A (and inv)\n",
    "\n",
    "ACB0 = np.zeros((ONE_SET_SIZE,3)).astype('int')    # One needs 3 different assignments since the arrays would be linked otherwise.\n",
    "BAC1 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "BCA0 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "\n",
    "ACB0[:,0] = ABC1[:,0]\n",
    "BAC1[:,1] = ABC1[:,0]\n",
    "BCA0[:,2] = ABC1[:,0]\n",
    "\n",
    "ACB0[:,2] = ABC1[:,1]\n",
    "BAC1[:,0] = ABC1[:,1]\n",
    "BCA0[:,0] = ABC1[:,1]\n",
    "\n",
    "ACB0[:,1] = ABC1[:,2]\n",
    "BAC1[:,2] = ABC1[:,2]\n",
    "BCA0[:,1] = ABC1[:,2]\n",
    "\n",
    "#assemble everything into one huge pile of S*** in order to shuffle it\n",
    "id_fin = np.concatenate((ABC1, BAC1, ACB0, BCA0),axis=0)\n",
    "labels = np.concatenate((data_y1,data_y0),axis=0)\n",
    "id_labels = np.concatenate((id_fin,labels),axis=1)\n",
    "\n",
    "data_nn_x = np.zeros((ONE_SET_SIZE * 4,FEATURE_SIZE * 3))\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,ONE_SET_SIZE * 4):\n",
    "            data_nn_x[j,range(i*FEATURE_SIZE, FEATURE_SIZE*(i+1))] = load_features(id_labels[j,i].astype('int'))\n",
    "            if j%50000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "\n",
    "#print(train_trip)\n",
    "#print(train_trip_inv)\n",
    "#now we try to randomize the data in order to increase our accuracy\n",
    "# we may even double the dataset by using 0 as well as the 1 case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#np.savetxt(\"data_nn_x.csv\", data_nn_x, delimiter=',')\n",
    "data_nn_x.astype('float16')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "collab = np.concatenate((data_nn_x,labels), axis=1, dtype='float16')\n",
    "np.random.shuffle(collab)\n",
    "\n",
    "data_x = collab[:,:FEATURE_SIZE * 3]\n",
    "data_y = collab[:,FEATURE_SIZE * 3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets shuffle the training data:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(\"%d bytes\" % (data_nn_x.size * data_nn_x.itemsize))\n",
    "#np.savetxt(\"nn_data.csv\",data_nn_x,delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel data_nn_x\n",
    "print(data_x.dtype)\n",
    "print(data_y.dtype)\n",
    "\n",
    "print(np.max(data_x))\n",
    "print(np.min(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#init=uniform\n",
    "#TAKE IT Branda:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(4096, activation='relu', input_dim=FEATURE_SIZE * 3, kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.5))\n",
    "#neuralNetwork.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "#neuralNetwork.add(Dropout(0.4))\n",
    "neuralNetwork.add(Dense(256, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(64, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.2))\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "neuralNetwork.summary()\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(data_x.astype('float16'), data_y.astype('float16'), epochs=128, batch_size=1024, verbose=2, validation_split = 0.2)\n",
    "\n",
    "#evaluation of the network prediction\n",
    "#predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "#predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "#print(predict_test_nn)\n",
    "#print(predict_train_nn)\n",
    "#predict_test_nn = (predict_test_nn >= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Layers:                         F.      Epoch   Val_Acc\n",
    "4096-4096-1024-256-64-1         2048    64      0.7962\n",
    "4096-4096-1024-256-64-1         2048    128     0.8043\n",
    "6144-4096-2048-1024-256-64-1    2048    64      0.7943\n",
    "6144-4096-2048-1024-256-64-1    2048    256     0.8044\n",
    "4096-4096-1024-256-64-1         2048    64      0.8329 #using 0.1 initial dropout (score was super bad)\n",
    "4096-4096-1024-256-64-1         2048    64      0.7961 #0.5 Drop except for last 0.2\n",
    "6144-1024-256-64-1              2048    128     0.8031 #check the SVD value and retry again\n",
    "8162-256-64-1                   3072    256     0.732\n",
    "4096-256-64-1                   1024    128     0.718\n",
    "4096-256-64-1                   2048    64      0.804\n",
    "4096-256-64-1                   2560    64\n",
    "\n",
    "Lets try to find out what dim-red. is needed: 1024-0.5-64-0.2-1 32e\n",
    "Factor:         Val_Acc:\n",
    "3072            0.725\n",
    "2048            0.796 #wtf is this even.\n",
    "1024            0.712"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_trip = np.loadtxt(\"test_triplets.txt\").astype('int')\n",
    "test_nn_x = np.zeros((59544,FEATURE_SIZE * 3))\n",
    "for i in range(0,3):\n",
    "        for j in range(0,59544):\n",
    "            test_nn_x[j,range(i*FEATURE_SIZE , FEATURE_SIZE*(i+1))] = load_features(test_trip[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "predict_test_nn = neuralNetwork.predict(test_nn_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% gotta try some shit:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export = np.array(predict_test_nn>0.5).astype('int')\n",
    "np.savetxt(\"submission_retrain_v8.txt\",np.round(export,decimals=0), fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% export prediction:\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}