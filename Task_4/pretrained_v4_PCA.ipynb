{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import layers\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "import sklearn as skl\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "model = VGG16(include_top=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "def get_features(ID):\n",
    "    img_path = 'food_porn/food/' + ID + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = np.array(model.predict(x))\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.zeros((10000,1001))\n",
    "id = np.array(range(0,10000))\n",
    "data[:,0] = id.transpose()\n",
    "\n",
    "def features():\n",
    "    for i in range(0,10000):\n",
    "        featureid = '{0:05}'.format(i)\n",
    "        features = get_features(featureid)\n",
    "        data[i,1:] = features\n",
    "        if i % 100 == True:\n",
    "            print(i)\n",
    "    return data\n",
    "\n",
    "#data = features()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets read out all the features of the dataset:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_ass = 'features_vgg16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "#print(np.count_nonzero(data))\n",
    "#save_ass = 'features_vgg16.csv'\n",
    "\n",
    "#np.savetxt(save_ass, data, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using false we got:     527 correct\n",
    "using true we got:      520 correct\n",
    "\n",
    "how about... we use the true version to feed a NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "train_trip = np.loadtxt(\"train_triplets.txt\").astype('int')\n",
    "\n",
    "data = np.genfromtxt('features_vgg16.csv', delimiter= ',')\n",
    "data = np.array(data[:,1:])\n",
    "\n",
    "reduction = 256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets prepare the data for the NN\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "temp_data = np.round(data,decimals=4)\n",
    "temp =0\n",
    "for i in range(0,1000):\n",
    "    if np.count_nonzero(temp_data[:,i])==0:\n",
    "        print(\"Empty Column:\", i)\n",
    "        temp=temp+1\n",
    "print(temp)\n",
    "\n",
    "for i in range(10000):\n",
    "    if np.count_nonzero(temp_data[i,:])==0:\n",
    "        print(\"Empty Row:\", i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "svd = TruncatedSVD(n_components=reduction, algorithm='arpack')\n",
    "svd.fit(data)\n",
    "transformed = svd.transform(data)\n",
    "print(transformed.shape)\n",
    "\n",
    "def load_features(int_id):\n",
    "    return transformed[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 512)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=reduction, svd_solver='arpack')\n",
    "pca.fit(data)\n",
    "transformed = pca.transform(data)\n",
    "print(transformed.shape)\n",
    "\n",
    "def load_features(int_id):\n",
    "    return transformed[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 10000\n",
      "we're at entry: 0 20000\n",
      "we're at entry: 0 30000\n",
      "we're at entry: 0 40000\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 0 60000\n",
      "we're at entry: 0 70000\n",
      "we're at entry: 0 80000\n",
      "we're at entry: 0 90000\n",
      "we're at entry: 0 100000\n",
      "we're at entry: 0 110000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 10000\n",
      "we're at entry: 1 20000\n",
      "we're at entry: 1 30000\n",
      "we're at entry: 1 40000\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 1 60000\n",
      "we're at entry: 1 70000\n",
      "we're at entry: 1 80000\n",
      "we're at entry: 1 90000\n",
      "we're at entry: 1 100000\n",
      "we're at entry: 1 110000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 10000\n",
      "we're at entry: 2 20000\n",
      "we're at entry: 2 30000\n",
      "we're at entry: 2 40000\n",
      "we're at entry: 2 50000\n",
      "we're at entry: 2 60000\n",
      "we're at entry: 2 70000\n",
      "we're at entry: 2 80000\n",
      "we're at entry: 2 90000\n",
      "we're at entry: 2 100000\n",
      "we're at entry: 2 110000\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#create the base dataset with the 2 sets of labels\n",
    "data_y1 = np.ones((59515,1))\n",
    "data_y0 = np.zeros((59515,1))\n",
    "\n",
    "#create the inverse dataset:\n",
    "train_trip_inv = np.zeros((59515,3)).astype('int')\n",
    "train_trip_inv[:,0] = train_trip[:,0]\n",
    "train_trip_inv[:,1] = train_trip[:,2]\n",
    "train_trip_inv[:,2] = train_trip[:,1]\n",
    "\n",
    "#assemble everything into one huge pile of S*** in order to shuffle it\n",
    "id_fin = np.concatenate((train_trip,train_trip_inv),axis=0)\n",
    "labels = np.concatenate((data_y1,data_y0),axis=0)\n",
    "id_labels = np.concatenate((id_fin,labels),axis=1)\n",
    "\n",
    "data_nn_x = np.zeros((119030,3*reduction))\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,119030):\n",
    "            data_nn_x[j,range(i*reduction,reduction*(i+1))] = load_features(id_labels[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "\n",
    "#print(train_trip)\n",
    "#print(train_trip_inv)\n",
    "#now we try to randomize the data in order to increase our accuracy\n",
    "# we may even double the dataset by using 0 as well as the 1 case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "collab = np.concatenate((data_nn_x,labels), axis=1)\n",
    "np.random.shuffle(collab)\n",
    "\n",
    "data_x = collab[:,:3*reduction]\n",
    "data_y = collab[:,3*reduction]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets shuffle the training data:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "#print(\"%d bytes\" % (data_nn_x.size * data_nn_x.itemsize))\n",
    "#np.savetxt(\"nn_data.csv\",data_nn_x,delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "layer = layers.Dense(\n",
    "    units=64,\n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-2, l2=1e-2),\n",
    "    bias_regularizer=regularizers.l2(1e-2),\n",
    "    activity_regularizer=regularizers.l2(1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119030, 1536)\n",
      "-0.6010278384028279\n",
      "0.9335400281491291\n",
      "5.918253423436879e-06\n",
      "92005814\n",
      "182830080\n"
     ]
    }
   ],
   "source": [
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#normalize the data:\n",
    "print(data_x.shape)\n",
    "print(np.min(data_x))\n",
    "print(np.max(data_x))\n",
    "print(np.mean(data_x))\n",
    "print(np.count_nonzero(data_x<0))\n",
    "print(np.count_nonzero(data_x))\n",
    "\n",
    "temp_data = np.round(data_x,decimals=1)\n",
    "data_x = np.round(data_x,decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "normalise data in order to improve initial layer\n",
    "\n",
    "perform dimensionality reduction on input set - DONE\n",
    "\n",
    "test out different optimizers like SGD, adam etc.\n",
    "\n",
    "there are no empty columns in the data (decimals=4) - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "93/93 - 1s - loss: 4.0708 - accuracy: 0.5022 - val_loss: 3.1744 - val_accuracy: 0.4975\n",
      "Epoch 2/128\n",
      "93/93 - 1s - loss: 2.5885 - accuracy: 0.5033 - val_loss: 2.0640 - val_accuracy: 0.4975\n",
      "Epoch 3/128\n",
      "93/93 - 1s - loss: 1.7044 - accuracy: 0.5088 - val_loss: 1.4012 - val_accuracy: 0.4975\n",
      "Epoch 4/128\n",
      "93/93 - 1s - loss: 1.1990 - accuracy: 0.5292 - val_loss: 1.0454 - val_accuracy: 0.4975\n",
      "Epoch 5/128\n",
      "93/93 - 1s - loss: 0.9268 - accuracy: 0.5800 - val_loss: 0.8866 - val_accuracy: 0.4975\n",
      "Epoch 6/128\n",
      "93/93 - 1s - loss: 0.7867 - accuracy: 0.6289 - val_loss: 0.7914 - val_accuracy: 0.5200\n",
      "Epoch 7/128\n",
      "93/93 - 1s - loss: 0.7244 - accuracy: 0.6437 - val_loss: 0.7601 - val_accuracy: 0.5379\n",
      "Epoch 8/128\n",
      "93/93 - 1s - loss: 0.6982 - accuracy: 0.6521 - val_loss: 0.7147 - val_accuracy: 0.6420\n",
      "Epoch 9/128\n",
      "93/93 - 1s - loss: 0.6886 - accuracy: 0.6557 - val_loss: 0.7023 - val_accuracy: 0.6501\n",
      "Epoch 10/128\n",
      "93/93 - 1s - loss: 0.6846 - accuracy: 0.6634 - val_loss: 0.6899 - val_accuracy: 0.6596\n",
      "Epoch 11/128\n",
      "93/93 - 1s - loss: 0.6864 - accuracy: 0.6666 - val_loss: 0.6854 - val_accuracy: 0.6697\n",
      "Epoch 12/128\n",
      "93/93 - 1s - loss: 0.6864 - accuracy: 0.6677 - val_loss: 0.6822 - val_accuracy: 0.6687\n",
      "Epoch 13/128\n",
      "93/93 - 1s - loss: 0.6864 - accuracy: 0.6706 - val_loss: 0.6862 - val_accuracy: 0.6671\n",
      "Epoch 14/128\n",
      "93/93 - 1s - loss: 0.6863 - accuracy: 0.6731 - val_loss: 0.6891 - val_accuracy: 0.6682\n",
      "Epoch 15/128\n",
      "93/93 - 1s - loss: 0.6889 - accuracy: 0.6743 - val_loss: 0.6898 - val_accuracy: 0.6715\n",
      "Epoch 16/128\n",
      "93/93 - 1s - loss: 0.6889 - accuracy: 0.6772 - val_loss: 0.6825 - val_accuracy: 0.6783\n",
      "Epoch 17/128\n",
      "93/93 - 1s - loss: 0.6868 - accuracy: 0.6775 - val_loss: 0.6908 - val_accuracy: 0.6715\n",
      "Epoch 18/128\n",
      "93/93 - 1s - loss: 0.6861 - accuracy: 0.6805 - val_loss: 0.6872 - val_accuracy: 0.6714\n",
      "Epoch 19/128\n",
      "93/93 - 1s - loss: 0.6864 - accuracy: 0.6834 - val_loss: 0.6877 - val_accuracy: 0.6790\n",
      "Epoch 20/128\n",
      "93/93 - 1s - loss: 0.6865 - accuracy: 0.6845 - val_loss: 0.6885 - val_accuracy: 0.6838\n",
      "Epoch 21/128\n",
      "93/93 - 1s - loss: 0.6851 - accuracy: 0.6880 - val_loss: 0.6870 - val_accuracy: 0.6844\n",
      "Epoch 22/128\n",
      "93/93 - 1s - loss: 0.6856 - accuracy: 0.6875 - val_loss: 0.6868 - val_accuracy: 0.6812\n",
      "Epoch 23/128\n",
      "93/93 - 1s - loss: 0.6836 - accuracy: 0.6879 - val_loss: 0.6898 - val_accuracy: 0.6840\n",
      "Epoch 24/128\n",
      "93/93 - 1s - loss: 0.6811 - accuracy: 0.6905 - val_loss: 0.6887 - val_accuracy: 0.6867\n",
      "Epoch 25/128\n",
      "93/93 - 1s - loss: 0.6801 - accuracy: 0.6913 - val_loss: 0.6856 - val_accuracy: 0.6854\n",
      "Epoch 26/128\n",
      "93/93 - 1s - loss: 0.6829 - accuracy: 0.6918 - val_loss: 0.6845 - val_accuracy: 0.6858\n",
      "Epoch 27/128\n",
      "93/93 - 1s - loss: 0.6807 - accuracy: 0.6945 - val_loss: 0.6842 - val_accuracy: 0.6906\n",
      "Epoch 28/128\n",
      "93/93 - 1s - loss: 0.6814 - accuracy: 0.6928 - val_loss: 0.6897 - val_accuracy: 0.6826\n",
      "Epoch 29/128\n",
      "93/93 - 1s - loss: 0.6806 - accuracy: 0.6964 - val_loss: 0.6871 - val_accuracy: 0.6875\n",
      "Epoch 30/128\n",
      "93/93 - 1s - loss: 0.6795 - accuracy: 0.6967 - val_loss: 0.6824 - val_accuracy: 0.6910\n",
      "Epoch 31/128\n",
      "93/93 - 1s - loss: 0.6782 - accuracy: 0.6969 - val_loss: 0.6825 - val_accuracy: 0.6905\n",
      "Epoch 32/128\n",
      "93/93 - 1s - loss: 0.6773 - accuracy: 0.6989 - val_loss: 0.6801 - val_accuracy: 0.6923\n",
      "Epoch 33/128\n",
      "93/93 - 1s - loss: 0.6785 - accuracy: 0.6986 - val_loss: 0.6831 - val_accuracy: 0.6905\n",
      "Epoch 34/128\n",
      "93/93 - 1s - loss: 0.6778 - accuracy: 0.6991 - val_loss: 0.6848 - val_accuracy: 0.6871\n",
      "Epoch 35/128\n",
      "93/93 - 1s - loss: 0.6748 - accuracy: 0.7023 - val_loss: 0.6841 - val_accuracy: 0.6870\n",
      "Epoch 36/128\n",
      "93/93 - 1s - loss: 0.6741 - accuracy: 0.6992 - val_loss: 0.6800 - val_accuracy: 0.6950\n",
      "Epoch 37/128\n",
      "93/93 - 1s - loss: 0.6750 - accuracy: 0.7010 - val_loss: 0.6797 - val_accuracy: 0.6927\n",
      "Epoch 38/128\n",
      "93/93 - 1s - loss: 0.6733 - accuracy: 0.7023 - val_loss: 0.6784 - val_accuracy: 0.6942\n",
      "Epoch 39/128\n",
      "93/93 - 1s - loss: 0.6716 - accuracy: 0.7031 - val_loss: 0.6827 - val_accuracy: 0.6919\n",
      "Epoch 40/128\n",
      "93/93 - 1s - loss: 0.6724 - accuracy: 0.7045 - val_loss: 0.6806 - val_accuracy: 0.6965\n",
      "Epoch 41/128\n",
      "93/93 - 1s - loss: 0.6696 - accuracy: 0.7059 - val_loss: 0.6793 - val_accuracy: 0.6997\n",
      "Epoch 42/128\n",
      "93/93 - 1s - loss: 0.6712 - accuracy: 0.7049 - val_loss: 0.6737 - val_accuracy: 0.7007\n",
      "Epoch 43/128\n",
      "93/93 - 1s - loss: 0.6710 - accuracy: 0.7056 - val_loss: 0.6773 - val_accuracy: 0.6994\n",
      "Epoch 44/128\n",
      "93/93 - 1s - loss: 0.6677 - accuracy: 0.7070 - val_loss: 0.6772 - val_accuracy: 0.6932\n",
      "Epoch 45/128\n",
      "93/93 - 1s - loss: 0.6683 - accuracy: 0.7075 - val_loss: 0.6751 - val_accuracy: 0.7014\n",
      "Epoch 46/128\n",
      "93/93 - 1s - loss: 0.6659 - accuracy: 0.7099 - val_loss: 0.6802 - val_accuracy: 0.6955\n",
      "Epoch 47/128\n",
      "93/93 - 1s - loss: 0.6667 - accuracy: 0.7079 - val_loss: 0.6794 - val_accuracy: 0.6971\n",
      "Epoch 48/128\n",
      "93/93 - 1s - loss: 0.6638 - accuracy: 0.7094 - val_loss: 0.6781 - val_accuracy: 0.6985\n",
      "Epoch 49/128\n",
      "93/93 - 1s - loss: 0.6660 - accuracy: 0.7094 - val_loss: 0.6774 - val_accuracy: 0.6992\n",
      "Epoch 50/128\n",
      "93/93 - 1s - loss: 0.6639 - accuracy: 0.7103 - val_loss: 0.6795 - val_accuracy: 0.6967\n",
      "Epoch 51/128\n",
      "93/93 - 1s - loss: 0.6624 - accuracy: 0.7123 - val_loss: 0.6743 - val_accuracy: 0.6960\n",
      "Epoch 52/128\n",
      "93/93 - 1s - loss: 0.6632 - accuracy: 0.7110 - val_loss: 0.6706 - val_accuracy: 0.7030\n",
      "Epoch 53/128\n",
      "93/93 - 1s - loss: 0.6635 - accuracy: 0.7103 - val_loss: 0.6706 - val_accuracy: 0.7018\n",
      "Epoch 54/128\n",
      "93/93 - 1s - loss: 0.6589 - accuracy: 0.7128 - val_loss: 0.6743 - val_accuracy: 0.6972\n",
      "Epoch 55/128\n",
      "93/93 - 1s - loss: 0.6595 - accuracy: 0.7131 - val_loss: 0.6725 - val_accuracy: 0.7013\n",
      "Epoch 56/128\n",
      "93/93 - 1s - loss: 0.6605 - accuracy: 0.7150 - val_loss: 0.6741 - val_accuracy: 0.7032\n",
      "Epoch 57/128\n",
      "93/93 - 1s - loss: 0.6595 - accuracy: 0.7140 - val_loss: 0.6736 - val_accuracy: 0.7013\n",
      "Epoch 58/128\n",
      "93/93 - 1s - loss: 0.6587 - accuracy: 0.7163 - val_loss: 0.6746 - val_accuracy: 0.7016\n",
      "Epoch 59/128\n",
      "93/93 - 1s - loss: 0.6594 - accuracy: 0.7139 - val_loss: 0.6722 - val_accuracy: 0.7057\n",
      "Epoch 60/128\n",
      "93/93 - 1s - loss: 0.6570 - accuracy: 0.7154 - val_loss: 0.6731 - val_accuracy: 0.6944\n",
      "Epoch 61/128\n",
      "93/93 - 1s - loss: 0.6577 - accuracy: 0.7161 - val_loss: 0.6717 - val_accuracy: 0.7070\n",
      "Epoch 62/128\n",
      "93/93 - 1s - loss: 0.6587 - accuracy: 0.7151 - val_loss: 0.6684 - val_accuracy: 0.7078\n",
      "Epoch 63/128\n",
      "93/93 - 1s - loss: 0.6572 - accuracy: 0.7158 - val_loss: 0.6736 - val_accuracy: 0.6997\n",
      "Epoch 64/128\n",
      "93/93 - 1s - loss: 0.6560 - accuracy: 0.7153 - val_loss: 0.6657 - val_accuracy: 0.7050\n",
      "Epoch 65/128\n",
      "93/93 - 1s - loss: 0.6552 - accuracy: 0.7162 - val_loss: 0.6659 - val_accuracy: 0.7043\n",
      "Epoch 66/128\n",
      "93/93 - 1s - loss: 0.6565 - accuracy: 0.7172 - val_loss: 0.6718 - val_accuracy: 0.7014\n",
      "Epoch 67/128\n",
      "93/93 - 1s - loss: 0.6528 - accuracy: 0.7198 - val_loss: 0.6709 - val_accuracy: 0.7040\n",
      "Epoch 68/128\n",
      "93/93 - 1s - loss: 0.6542 - accuracy: 0.7202 - val_loss: 0.6670 - val_accuracy: 0.7055\n",
      "Epoch 69/128\n",
      "93/93 - 1s - loss: 0.6525 - accuracy: 0.7206 - val_loss: 0.6665 - val_accuracy: 0.7084\n",
      "Epoch 70/128\n",
      "93/93 - 1s - loss: 0.6526 - accuracy: 0.7189 - val_loss: 0.6687 - val_accuracy: 0.6992\n",
      "Epoch 71/128\n",
      "93/93 - 1s - loss: 0.6533 - accuracy: 0.7167 - val_loss: 0.6643 - val_accuracy: 0.7084\n",
      "Epoch 72/128\n",
      "93/93 - 1s - loss: 0.6512 - accuracy: 0.7206 - val_loss: 0.6641 - val_accuracy: 0.7046\n",
      "Epoch 73/128\n",
      "93/93 - 1s - loss: 0.6519 - accuracy: 0.7221 - val_loss: 0.6660 - val_accuracy: 0.7100\n",
      "Epoch 74/128\n",
      "93/93 - 1s - loss: 0.6531 - accuracy: 0.7195 - val_loss: 0.6651 - val_accuracy: 0.7044\n",
      "Epoch 75/128\n",
      "93/93 - 1s - loss: 0.6497 - accuracy: 0.7218 - val_loss: 0.6613 - val_accuracy: 0.7083\n",
      "Epoch 76/128\n",
      "93/93 - 1s - loss: 0.6484 - accuracy: 0.7230 - val_loss: 0.6685 - val_accuracy: 0.7054\n",
      "Epoch 77/128\n",
      "93/93 - 1s - loss: 0.6501 - accuracy: 0.7233 - val_loss: 0.6684 - val_accuracy: 0.7097\n",
      "Epoch 78/128\n",
      "93/93 - 1s - loss: 0.6514 - accuracy: 0.7194 - val_loss: 0.6623 - val_accuracy: 0.7099\n",
      "Epoch 79/128\n",
      "93/93 - 1s - loss: 0.6508 - accuracy: 0.7227 - val_loss: 0.6642 - val_accuracy: 0.7087\n",
      "Epoch 80/128\n",
      "93/93 - 1s - loss: 0.6498 - accuracy: 0.7227 - val_loss: 0.6616 - val_accuracy: 0.7108\n",
      "Epoch 81/128\n",
      "93/93 - 1s - loss: 0.6483 - accuracy: 0.7238 - val_loss: 0.6607 - val_accuracy: 0.7128\n",
      "Epoch 82/128\n",
      "93/93 - 1s - loss: 0.6492 - accuracy: 0.7229 - val_loss: 0.6652 - val_accuracy: 0.7112\n",
      "Epoch 83/128\n",
      "93/93 - 1s - loss: 0.6503 - accuracy: 0.7217 - val_loss: 0.6605 - val_accuracy: 0.7062\n",
      "Epoch 84/128\n",
      "93/93 - 1s - loss: 0.6499 - accuracy: 0.7218 - val_loss: 0.6644 - val_accuracy: 0.7079\n",
      "Epoch 85/128\n",
      "93/93 - 1s - loss: 0.6494 - accuracy: 0.7227 - val_loss: 0.6655 - val_accuracy: 0.7042\n",
      "Epoch 86/128\n",
      "93/93 - 1s - loss: 0.6464 - accuracy: 0.7246 - val_loss: 0.6613 - val_accuracy: 0.7070\n",
      "Epoch 87/128\n",
      "93/93 - 1s - loss: 0.6466 - accuracy: 0.7225 - val_loss: 0.6645 - val_accuracy: 0.7122\n",
      "Epoch 88/128\n",
      "93/93 - 1s - loss: 0.6496 - accuracy: 0.7222 - val_loss: 0.6590 - val_accuracy: 0.7113\n",
      "Epoch 89/128\n",
      "93/93 - 1s - loss: 0.6467 - accuracy: 0.7244 - val_loss: 0.6614 - val_accuracy: 0.7112\n",
      "Epoch 90/128\n",
      "93/93 - 1s - loss: 0.6483 - accuracy: 0.7223 - val_loss: 0.6610 - val_accuracy: 0.7109\n",
      "Epoch 91/128\n",
      "93/93 - 1s - loss: 0.6483 - accuracy: 0.7244 - val_loss: 0.6664 - val_accuracy: 0.7057\n",
      "Epoch 92/128\n",
      "93/93 - 1s - loss: 0.6467 - accuracy: 0.7250 - val_loss: 0.6667 - val_accuracy: 0.7084\n",
      "Epoch 93/128\n",
      "93/93 - 1s - loss: 0.6481 - accuracy: 0.7235 - val_loss: 0.6584 - val_accuracy: 0.7107\n",
      "Epoch 94/128\n",
      "93/93 - 1s - loss: 0.6459 - accuracy: 0.7243 - val_loss: 0.6595 - val_accuracy: 0.7118\n",
      "Epoch 95/128\n",
      "93/93 - 1s - loss: 0.6456 - accuracy: 0.7248 - val_loss: 0.6610 - val_accuracy: 0.7113\n",
      "Epoch 96/128\n",
      "93/93 - 1s - loss: 0.6440 - accuracy: 0.7281 - val_loss: 0.6622 - val_accuracy: 0.7144\n",
      "Epoch 97/128\n",
      "93/93 - 1s - loss: 0.6442 - accuracy: 0.7259 - val_loss: 0.6558 - val_accuracy: 0.7176\n",
      "Epoch 98/128\n",
      "93/93 - 1s - loss: 0.6455 - accuracy: 0.7252 - val_loss: 0.6660 - val_accuracy: 0.7023\n",
      "Epoch 99/128\n",
      "93/93 - 1s - loss: 0.6455 - accuracy: 0.7266 - val_loss: 0.6579 - val_accuracy: 0.7099\n",
      "Epoch 100/128\n",
      "93/93 - 1s - loss: 0.6453 - accuracy: 0.7267 - val_loss: 0.6615 - val_accuracy: 0.7120\n",
      "Epoch 101/128\n",
      "93/93 - 1s - loss: 0.6461 - accuracy: 0.7269 - val_loss: 0.6574 - val_accuracy: 0.7195\n",
      "Epoch 102/128\n",
      "93/93 - 1s - loss: 0.6434 - accuracy: 0.7261 - val_loss: 0.6562 - val_accuracy: 0.7135\n",
      "Epoch 103/128\n",
      "93/93 - 1s - loss: 0.6456 - accuracy: 0.7276 - val_loss: 0.6645 - val_accuracy: 0.7038\n",
      "Epoch 104/128\n",
      "93/93 - 1s - loss: 0.6460 - accuracy: 0.7253 - val_loss: 0.6619 - val_accuracy: 0.7140\n",
      "Epoch 105/128\n",
      "93/93 - 1s - loss: 0.6432 - accuracy: 0.7274 - val_loss: 0.6586 - val_accuracy: 0.7171\n",
      "Epoch 106/128\n",
      "93/93 - 1s - loss: 0.6487 - accuracy: 0.7246 - val_loss: 0.6586 - val_accuracy: 0.7141\n",
      "Epoch 107/128\n",
      "93/93 - 1s - loss: 0.6436 - accuracy: 0.7257 - val_loss: 0.6599 - val_accuracy: 0.7124\n",
      "Epoch 108/128\n",
      "93/93 - 1s - loss: 0.6457 - accuracy: 0.7281 - val_loss: 0.6589 - val_accuracy: 0.7152\n",
      "Epoch 109/128\n",
      "93/93 - 1s - loss: 0.6427 - accuracy: 0.7294 - val_loss: 0.6593 - val_accuracy: 0.7117\n",
      "Epoch 110/128\n",
      "93/93 - 1s - loss: 0.6424 - accuracy: 0.7275 - val_loss: 0.6535 - val_accuracy: 0.7187\n",
      "Epoch 111/128\n",
      "93/93 - 1s - loss: 0.6442 - accuracy: 0.7251 - val_loss: 0.6565 - val_accuracy: 0.7152\n",
      "Epoch 112/128\n",
      "93/93 - 1s - loss: 0.6427 - accuracy: 0.7283 - val_loss: 0.6585 - val_accuracy: 0.7186\n",
      "Epoch 113/128\n",
      "93/93 - 1s - loss: 0.6441 - accuracy: 0.7267 - val_loss: 0.6615 - val_accuracy: 0.7137\n",
      "Epoch 114/128\n",
      "93/93 - 1s - loss: 0.6413 - accuracy: 0.7281 - val_loss: 0.6562 - val_accuracy: 0.7172\n",
      "Epoch 115/128\n",
      "93/93 - 1s - loss: 0.6429 - accuracy: 0.7293 - val_loss: 0.6593 - val_accuracy: 0.7129\n",
      "Epoch 116/128\n",
      "93/93 - 1s - loss: 0.6418 - accuracy: 0.7284 - val_loss: 0.6536 - val_accuracy: 0.7171\n",
      "Epoch 117/128\n",
      "93/93 - 1s - loss: 0.6436 - accuracy: 0.7273 - val_loss: 0.6566 - val_accuracy: 0.7123\n",
      "Epoch 118/128\n",
      "93/93 - 1s - loss: 0.6428 - accuracy: 0.7292 - val_loss: 0.6607 - val_accuracy: 0.7165\n",
      "Epoch 119/128\n",
      "93/93 - 1s - loss: 0.6435 - accuracy: 0.7272 - val_loss: 0.6594 - val_accuracy: 0.7127\n",
      "Epoch 120/128\n",
      "93/93 - 1s - loss: 0.6434 - accuracy: 0.7280 - val_loss: 0.6570 - val_accuracy: 0.7165\n",
      "Epoch 121/128\n",
      "93/93 - 1s - loss: 0.6437 - accuracy: 0.7292 - val_loss: 0.6556 - val_accuracy: 0.7143\n",
      "Epoch 122/128\n",
      "93/93 - 1s - loss: 0.6412 - accuracy: 0.7283 - val_loss: 0.6547 - val_accuracy: 0.7130\n",
      "Epoch 123/128\n",
      "93/93 - 1s - loss: 0.6420 - accuracy: 0.7278 - val_loss: 0.6570 - val_accuracy: 0.7126\n",
      "Epoch 124/128\n",
      "93/93 - 1s - loss: 0.6427 - accuracy: 0.7267 - val_loss: 0.6587 - val_accuracy: 0.7110\n",
      "Epoch 125/128\n",
      "93/93 - 1s - loss: 0.6419 - accuracy: 0.7291 - val_loss: 0.6723 - val_accuracy: 0.7032\n",
      "Epoch 126/128\n",
      "93/93 - 1s - loss: 0.6443 - accuracy: 0.7257 - val_loss: 0.6574 - val_accuracy: 0.7123\n",
      "Epoch 127/128\n",
      "93/93 - 1s - loss: 0.6417 - accuracy: 0.7295 - val_loss: 0.6588 - val_accuracy: 0.7150\n",
      "Epoch 128/128\n",
      "93/93 - 1s - loss: 0.6411 - accuracy: 0.7275 - val_loss: 0.6568 - val_accuracy: 0.7155\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 639,681\n",
      "Trainable params: 637,153\n",
      "Non-trainable params: 2,528\n",
      "_________________________________________________________________\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#TAKE IT Branda:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(256, activation='relu', input_dim=3*reduction, kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.5))\n",
    "neuralNetwork.add(Dense(256, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.4))\n",
    "neuralNetwork.add(Dense(256, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.4))\n",
    "neuralNetwork.add(Dense(256, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(128, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(64, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.2))\n",
    "neuralNetwork.add(Dense(32, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(16, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(data_x, data_y, epochs=128, batch_size=1024, verbose=2, validation_split = 0.2)\n",
    "\n",
    "neuralNetwork.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 10000\n",
      "we're at entry: 0 20000\n",
      "we're at entry: 0 30000\n",
      "we're at entry: 0 40000\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 10000\n",
      "we're at entry: 1 20000\n",
      "we're at entry: 1 30000\n",
      "we're at entry: 1 40000\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 10000\n",
      "we're at entry: 2 20000\n",
      "we're at entry: 2 30000\n",
      "we're at entry: 2 40000\n",
      "we're at entry: 2 50000\n"
     ]
    }
   ],
   "source": [
    "test_trip = np.loadtxt(\"test_triplets.txt\").astype('int')\n",
    "test_nn_x = np.zeros((59544,3*reduction))\n",
    "for i in range(0,3):\n",
    "        for j in range(0,59544):\n",
    "            test_nn_x[j,range(i*reduction,reduction*(i+1))] = load_features(test_trip[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "predict_test_nn = neuralNetwork.predict(test_nn_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% gotta try some shit:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "export = np.array(predict_test_nn>0.5).astype('int')\n",
    "np.savetxt(\"submission_vgg16_PCA_v1.txt\",np.round(export,decimals=0), fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% export prediction:\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}