{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import class_weight\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "last layer output shape: (None, 4096)\n"
     ]
    }
   ],
   "source": [
    "#model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = VGG16(include_top=True)\n",
    "\n",
    "last_layer = model.get_layer('fc1')\n",
    "print('last layer output shape:', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "\n",
    "FEATURE_SIZE = 4096"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "cut_model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_features(ID):\n",
    "    img_path = 'food_porn/food/' + ID + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = np.array(cut_model.predict(x))\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 0us/step\n",
      "[[('n04476259', 'tray', 0.27396187), ('n03485794', 'handkerchief', 0.23914962), ('n03291819', 'envelope', 0.18719405), ('n03871628', 'packet', 0.06075603), ('n03961711', 'plate_rack', 0.026308354)]]\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(decode_predictions(get_features('00000')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n",
      "801\n",
      "901\n",
      "1001\n",
      "1101\n",
      "1201\n",
      "1301\n",
      "1401\n",
      "1501\n",
      "1601\n",
      "1701\n",
      "1801\n",
      "1901\n",
      "2001\n",
      "2101\n",
      "2201\n",
      "2301\n",
      "2401\n",
      "2501\n",
      "2601\n",
      "2701\n",
      "2801\n",
      "2901\n",
      "3001\n",
      "3101\n",
      "3201\n",
      "3301\n",
      "3401\n",
      "3501\n",
      "3601\n",
      "3701\n",
      "3801\n",
      "3901\n",
      "4001\n",
      "4101\n",
      "4201\n",
      "4301\n",
      "4401\n",
      "4501\n",
      "4601\n",
      "4701\n",
      "4801\n",
      "4901\n",
      "5001\n",
      "5101\n",
      "5201\n",
      "5301\n",
      "5401\n",
      "5501\n",
      "5601\n",
      "5701\n",
      "5801\n",
      "5901\n",
      "6001\n",
      "6101\n",
      "6201\n",
      "6301\n",
      "6401\n",
      "6501\n",
      "6601\n",
      "6701\n",
      "6801\n",
      "6901\n",
      "7001\n",
      "7101\n",
      "7201\n",
      "7301\n",
      "7401\n",
      "7501\n",
      "7601\n",
      "7701\n",
      "7801\n",
      "7901\n",
      "8001\n",
      "8101\n",
      "8201\n",
      "8301\n",
      "8401\n",
      "8501\n",
      "8601\n",
      "8701\n",
      "8801\n",
      "8901\n",
      "9001\n",
      "9101\n",
      "9201\n",
      "9301\n",
      "9401\n",
      "9501\n",
      "9601\n",
      "9701\n",
      "9801\n",
      "9901\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.zeros((10000,FEATURE_SIZE + 1))\n",
    "id = np.array(range(0,10000))\n",
    "data[:,0] = id.transpose()\n",
    "\n",
    "def features():\n",
    "    for i in range(0,10000):\n",
    "        featureid = '{0:05}'.format(i)\n",
    "        features = get_features(featureid)\n",
    "        data[i,1:] = features\n",
    "        if i % 100 == True:\n",
    "            print(i)\n",
    "    return data\n",
    "\n",
    "data = features()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets read out all the features of the dataset:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ass = 'features_vgg16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10009999\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(data))\n",
    "save_ass = 'features_vgg16.csv'\n",
    "\n",
    "np.savetxt(save_ass, data, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using false we got:     527 correct\n",
    "using true we got:      520 correct\n",
    "\n",
    "how about... we use the true version to feed a NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int') \n",
    "\n",
    "data = np.genfromtxt(save_ass, delimiter= ',')\n",
    "data = np.array(data[:,1:])\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets prepare the data for the NN\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 0 100000\n",
      "we're at entry: 0 150000\n",
      "we're at entry: 0 200000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 1 100000\n",
      "we're at entry: 1 150000\n",
      "we're at entry: 1 200000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 50000\n",
      "we're at entry: 2 100000\n",
      "we're at entry: 2 150000\n",
      "we're at entry: 2 200000\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# There are 4 different permutations of ABC that we can know the answer of. (CAB and CBA we don't know)\n",
    "ONE_SET_SIZE = 59515\n",
    "\n",
    "#create the base dataset with the 2 sets of labels\n",
    "data_y1 = np.ones((ONE_SET_SIZE * 2,1))\n",
    "data_y0 = np.zeros((ONE_SET_SIZE * 2,1))\n",
    "\n",
    "#create the inverse dataset and a set where B is connected to A (and inv)\n",
    "\n",
    "ACB0 = np.zeros((ONE_SET_SIZE,3)).astype('int')    # One needs 3 different assignments since the arrays would be linked otherwise.\n",
    "BAC1 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "BCA0 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "\n",
    "ACB0[:,0] = ABC1[:,0]\n",
    "BAC1[:,1] = ABC1[:,0]\n",
    "BCA0[:,2] = ABC1[:,0]\n",
    "\n",
    "ACB0[:,2] = ABC1[:,1]\n",
    "BAC1[:,0] = ABC1[:,1]\n",
    "BCA0[:,0] = ABC1[:,1]\n",
    "\n",
    "ACB0[:,1] = ABC1[:,2]\n",
    "BAC1[:,2] = ABC1[:,2]\n",
    "BCA0[:,1] = ABC1[:,2]\n",
    "\n",
    "#assemble everything into one huge pile of S*** in order to shuffle it\n",
    "id_fin = np.concatenate((ABC1, BAC1, ACB0, BCA0),axis=0)\n",
    "labels = np.concatenate((data_y1,data_y0),axis=0)\n",
    "id_labels = np.concatenate((id_fin,labels),axis=1)\n",
    "\n",
    "data_nn_x = np.zeros((ONE_SET_SIZE * 4,FEATURE_SIZE * 3))\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,ONE_SET_SIZE * 4):\n",
    "            data_nn_x[j,range(i*FEATURE_SIZE, FEATURE_SIZE*(i+1))] = load_features(id_labels[j,i].astype('int'))\n",
    "            if j%50000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "\n",
    "#print(train_trip)\n",
    "#print(train_trip_inv)\n",
    "#now we try to randomize the data in order to increase our accuracy\n",
    "# we may even double the dataset by using 0 as well as the 1 case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collab = np.concatenate((data_nn_x,labels), axis=1)\n",
    "np.random.shuffle(collab)\n",
    "\n",
    "data_x = collab[:,:FEATURE_SIZE * 3]\n",
    "data_y = collab[:,FEATURE_SIZE * 3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets shuffle the training data:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(\"%d bytes\" % (data_nn_x.size * data_nn_x.itemsize))\n",
    "#np.savetxt(\"nn_data.csv\",data_nn_x,delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers.Dense(\n",
    "    units=64,\n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-2, l2=1e-2),\n",
    "    bias_regularizer=regularizers.l2(1e-2),\n",
    "    activity_regularizer=regularizers.l2(1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#TAKE IT Branda:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(4096, activation='relu', input_dim=FEATURE_SIZE * 3, kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.5))\n",
    "neuralNetwork.add(Dense(512, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.4))\n",
    "neuralNetwork.add(Dense(128, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(64, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.2))\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(data_x, data_y, epochs=128, batch_size=1024, verbose=1, validation_split = 0.2)\n",
    "\n",
    "#evaluation of the network prediction\n",
    "#predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "#predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "#print(predict_test_nn)\n",
    "#print(predict_train_nn)\n",
    "#predict_test_nn = (predict_test_nn >= 0.5)\n",
    "neuralNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 10000\n",
      "we're at entry: 0 20000\n",
      "we're at entry: 0 30000\n",
      "we're at entry: 0 40000\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 10000\n",
      "we're at entry: 1 20000\n",
      "we're at entry: 1 30000\n",
      "we're at entry: 1 40000\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 10000\n",
      "we're at entry: 2 20000\n",
      "we're at entry: 2 30000\n",
      "we're at entry: 2 40000\n",
      "we're at entry: 2 50000\n"
     ]
    }
   ],
   "source": [
    "test_trip = np.loadtxt(\"test_triplets.txt\").astype('int')\n",
    "test_nn_x = np.zeros((59544,FEATURE_SIZE * 3))\n",
    "for i in range(0,3):\n",
    "        for j in range(0,59544):\n",
    "            test_nn_x[j,range(i*FEATURE_SIZE , FEATURE_SIZE*(i+1))] = load_features(test_trip[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "predict_test_nn = neuralNetwork.predict(test_nn_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% gotta try some shit:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "export = np.array(predict_test_nn>0.5).astype('int')\n",
    "np.savetxt(\"submission_vgg16_regularizer.txt\",np.round(export,decimals=0), fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% export prediction:\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0035816c817341503006e0a8714912042b4c6673b969b2520b787454674f444b8",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}