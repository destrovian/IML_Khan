{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.utils import class_weight\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = VGG16(include_top=False, pooling='max')\n",
    "\n",
    "#last_layer = model.get_layer('fc1')\n",
    "#print('last layer output shape:', last_layer.output_shape)\n",
    "#last_output = last_layer.output\n",
    "\n",
    "FEATURE_SIZE = 512\n",
    "reduction = 512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in model.layers:\n",
    "#    layer._name = layer._name + str('_C')\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "#x = layers.Flatten()(last_output)\n",
    "#cut_model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_features(ID):\n",
    "    img_path = 'food_porn/food/' + ID + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(244, 244))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = np.array(model.predict(x))\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(get_features('00000').shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 990 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = np.zeros((10000,512))\n",
    "id = np.array(range(0,10000))\n",
    "\n",
    "def features():\n",
    "    for i in range(0,10000):\n",
    "        featureid = '{0:05}'.format(i)\n",
    "        features = get_features(featureid)\n",
    "        data[i,:] = features\n",
    "        if i % 100 == True:\n",
    "            print(i)\n",
    "    return data\n",
    "\n",
    "#data = features()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets read out all the features of the dataset:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_ass = 'features_vgg16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#print(np.count_nonzero(data))\n",
    "save_ass = 'features_vgg16_poolavg.csv'\n",
    "\n",
    "#np.savetxt(save_ass, data, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "using false we got:     527 correct\n",
    "using true we got:      520 correct\n",
    "\n",
    "how about... we use the true version to feed a NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#lets prepare the data for the NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int') \n",
    "\n",
    "data = np.genfromtxt(save_ass, delimiter= ',')\n",
    "#data = np.array(data[:,1:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.29 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int')\n",
    "\n",
    "#svd = TruncatedSVD(n_components=reduction)\n",
    "#svd.fit(data)\n",
    "#data = svd.transform(data)\n",
    "#print(data.shape)\n",
    "#np.savetxt(\"transformed_1024_ResV2.csv\", data, delimiter=',')\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ABC1 = np.loadtxt(\"train_triplets.txt\").astype('int')\n",
    "\n",
    "data = np.genfromtxt(\"transformed_2k_vgg16.csv\", delimiter= ',')\n",
    "data = np.array(data)\n",
    "#data = np.array(data[:,1:])\n",
    "\n",
    "def load_features(int_id):\n",
    "    return data[int_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "data2 = np.genfromtxt(\"transformed_2048_svd_arc_vgg16.csv\", delimiter= ',')\n",
    "data3 = np.genfromtxt(\"transformed_2048_svd_rand_vgg16.csv\", delimiter= ',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(np.max(data2))\n",
    "print(np.min(data2))\n",
    "print(np.count_nonzero(data2>0)/10000)\n",
    "print(data2)\n",
    "\n",
    "print(np.max(data3))\n",
    "print(np.min(data3))\n",
    "print(np.count_nonzero(data3>0)/10000)\n",
    "print(data3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 0 100000\n",
      "we're at entry: 0 150000\n",
      "we're at entry: 0 200000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 1 100000\n",
      "we're at entry: 1 150000\n",
      "we're at entry: 1 200000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 50000\n",
      "we're at entry: 2 100000\n",
      "we're at entry: 2 150000\n",
      "we're at entry: 2 200000\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# There are 4 different permutations of ABC that we can know the answer of. (CAB and CBA we don't know)\n",
    "ONE_SET_SIZE = 59515\n",
    "\n",
    "#create the base dataset with the 2 sets of labels\n",
    "data_y1 = np.ones((ONE_SET_SIZE * 2,1))\n",
    "data_y0 = np.zeros((ONE_SET_SIZE * 2,1))\n",
    "\n",
    "#create the inverse dataset and a set where B is connected to A (and inv)\n",
    "\n",
    "ACB0 = np.zeros((ONE_SET_SIZE,3)).astype('int')    # One needs 3 different assignments since the arrays would be linked otherwise.\n",
    "BAC1 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "BCA0 = np.zeros((ONE_SET_SIZE,3)).astype('int')\n",
    "\n",
    "ACB0[:,0] = ABC1[:,0]\n",
    "BAC1[:,1] = ABC1[:,0]\n",
    "BCA0[:,2] = ABC1[:,0]\n",
    "\n",
    "ACB0[:,2] = ABC1[:,1]\n",
    "BAC1[:,0] = ABC1[:,1]\n",
    "BCA0[:,0] = ABC1[:,1]\n",
    "\n",
    "ACB0[:,1] = ABC1[:,2]\n",
    "BAC1[:,2] = ABC1[:,2]\n",
    "BCA0[:,1] = ABC1[:,2]\n",
    "\n",
    "#assemble everything into one huge pile of S*** in order to shuffle it\n",
    "id_fin = np.concatenate((ABC1, BAC1, ACB0, BCA0),axis=0)\n",
    "labels = np.concatenate((data_y1,data_y0),axis=0)\n",
    "id_labels = np.concatenate((id_fin,labels),axis=1)\n",
    "\n",
    "data_nn_x = np.zeros((ONE_SET_SIZE * 4,FEATURE_SIZE * 3))\n",
    "\n",
    "for i in range(0,3):\n",
    "        for j in range(0,ONE_SET_SIZE * 4):\n",
    "            data_nn_x[j,range(i*FEATURE_SIZE, FEATURE_SIZE*(i+1))] = load_features(id_labels[j,i].astype('int'))\n",
    "            if j%50000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "\n",
    "#print(train_trip)\n",
    "#print(train_trip_inv)\n",
    "#now we try to randomize the data in order to increase our accuracy\n",
    "# we may even double the dataset by using 0 as well as the 1 case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#np.savetxt(\"data_nn_x.csv\", data_nn_x, delimiter=',')\n",
    "#data_nn_x.astype('float16')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collab = np.concatenate((data_nn_x,labels), axis=1) #dtype='float16'\n",
    "np.random.shuffle(collab)\n",
    "\n",
    "data_x = collab[:,:FEATURE_SIZE * 3]\n",
    "data_y = collab[:,FEATURE_SIZE * 3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% lets shuffle the training data:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#print(\"%d bytes\" % (data_nn_x.size * data_nn_x.itemsize))\n",
    "#np.savetxt(\"nn_data.csv\",data_nn_x,delimiter=',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "171.44268798828125\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "%xdel data_nn_x\n",
    "print(data_x.dtype)\n",
    "print(data_y.dtype)\n",
    "\n",
    "print(np.max(data_x))\n",
    "print(np.min(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when val_loss is no longer improving\n",
    "        monitor=\"loss\",\n",
    "        min_delta=0,\n",
    "        # Take the minimum of val_loss as a guideline.\n",
    "        mode=\"max\",\n",
    "        # No longer improving\" being further defined as \"for at least 50 epochs.\n",
    "        patience=300,\n",
    "        # Since we have such a large paitience we restore back to the best weights after a long waittime.\n",
    "        restore_best_weights = True,\n",
    "        verbose = 1,\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1536)              2360832   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,778,241\n",
      "Trainable params: 2,774,529\n",
      "Non-trainable params: 3,712\n",
      "_________________________________________________________________\n",
      "Epoch 1/256\n",
      "186/186 - 2s - loss: 2.9690 - accuracy: 0.6421 - val_loss: 1.1341 - val_accuracy: 0.6905\n",
      "Epoch 2/256\n",
      "186/186 - 2s - loss: 0.8868 - accuracy: 0.6834 - val_loss: 0.7678 - val_accuracy: 0.6721\n",
      "Epoch 3/256\n",
      "186/186 - 1s - loss: 0.7297 - accuracy: 0.6869 - val_loss: 0.7261 - val_accuracy: 0.6672\n",
      "Epoch 4/256\n",
      "186/186 - 1s - loss: 0.7015 - accuracy: 0.6873 - val_loss: 0.6989 - val_accuracy: 0.6752\n",
      "Epoch 5/256\n",
      "186/186 - 1s - loss: 0.6935 - accuracy: 0.6858 - val_loss: 0.7035 - val_accuracy: 0.6655\n",
      "Epoch 6/256\n",
      "186/186 - 1s - loss: 0.6922 - accuracy: 0.6860 - val_loss: 0.6913 - val_accuracy: 0.6821\n",
      "Epoch 7/256\n",
      "186/186 - 1s - loss: 0.6883 - accuracy: 0.6834 - val_loss: 0.6936 - val_accuracy: 0.6673\n",
      "Epoch 8/256\n",
      "186/186 - 1s - loss: 0.6872 - accuracy: 0.6848 - val_loss: 0.7160 - val_accuracy: 0.6618\n",
      "Epoch 9/256\n",
      "186/186 - 2s - loss: 0.6843 - accuracy: 0.6837 - val_loss: 0.7000 - val_accuracy: 0.6645\n",
      "Epoch 10/256\n",
      "186/186 - 1s - loss: 0.6821 - accuracy: 0.6853 - val_loss: 0.6860 - val_accuracy: 0.6749\n",
      "Epoch 11/256\n",
      "186/186 - 1s - loss: 0.6797 - accuracy: 0.6839 - val_loss: 0.6915 - val_accuracy: 0.6737\n",
      "Epoch 12/256\n",
      "186/186 - 1s - loss: 0.6781 - accuracy: 0.6863 - val_loss: 0.6807 - val_accuracy: 0.6733\n",
      "Epoch 13/256\n",
      "186/186 - 1s - loss: 0.6739 - accuracy: 0.6853 - val_loss: 0.6972 - val_accuracy: 0.6544\n",
      "Epoch 14/256\n",
      "186/186 - 1s - loss: 0.6735 - accuracy: 0.6851 - val_loss: 0.6954 - val_accuracy: 0.6500\n",
      "Epoch 15/256\n",
      "186/186 - 1s - loss: 0.6719 - accuracy: 0.6861 - val_loss: 0.7188 - val_accuracy: 0.6313\n",
      "Epoch 16/256\n",
      "186/186 - 1s - loss: 0.6681 - accuracy: 0.6856 - val_loss: 0.6628 - val_accuracy: 0.6862\n",
      "Epoch 17/256\n",
      "186/186 - 2s - loss: 0.6684 - accuracy: 0.6866 - val_loss: 0.6743 - val_accuracy: 0.6779\n",
      "Epoch 18/256\n",
      "186/186 - 1s - loss: 0.6654 - accuracy: 0.6863 - val_loss: 0.6714 - val_accuracy: 0.6796\n",
      "Epoch 19/256\n",
      "186/186 - 1s - loss: 0.6664 - accuracy: 0.6858 - val_loss: 0.6730 - val_accuracy: 0.6789\n",
      "Epoch 20/256\n",
      "186/186 - 1s - loss: 0.6658 - accuracy: 0.6863 - val_loss: 0.6795 - val_accuracy: 0.6732\n",
      "Epoch 21/256\n",
      "186/186 - 1s - loss: 0.6637 - accuracy: 0.6864 - val_loss: 0.7092 - val_accuracy: 0.6500\n",
      "Epoch 22/256\n",
      "186/186 - 1s - loss: 0.6624 - accuracy: 0.6865 - val_loss: 0.7199 - val_accuracy: 0.6319\n",
      "Epoch 23/256\n",
      "186/186 - 1s - loss: 0.6596 - accuracy: 0.6891 - val_loss: 0.6619 - val_accuracy: 0.6833\n",
      "Epoch 24/256\n",
      "186/186 - 1s - loss: 0.6609 - accuracy: 0.6881 - val_loss: 0.6857 - val_accuracy: 0.6632\n",
      "Epoch 25/256\n",
      "186/186 - 1s - loss: 0.6607 - accuracy: 0.6880 - val_loss: 0.6628 - val_accuracy: 0.6864\n",
      "Epoch 26/256\n",
      "186/186 - 1s - loss: 0.6599 - accuracy: 0.6875 - val_loss: 0.6664 - val_accuracy: 0.6777\n",
      "Epoch 27/256\n",
      "186/186 - 1s - loss: 0.6557 - accuracy: 0.6895 - val_loss: 0.7340 - val_accuracy: 0.6346\n",
      "Epoch 28/256\n",
      "186/186 - 1s - loss: 0.6536 - accuracy: 0.6885 - val_loss: 0.6474 - val_accuracy: 0.6909\n",
      "Epoch 29/256\n",
      "186/186 - 1s - loss: 0.6544 - accuracy: 0.6901 - val_loss: 0.6484 - val_accuracy: 0.6893\n",
      "Epoch 30/256\n",
      "186/186 - 1s - loss: 0.6536 - accuracy: 0.6889 - val_loss: 0.6822 - val_accuracy: 0.6451\n",
      "Epoch 31/256\n",
      "186/186 - 1s - loss: 0.6518 - accuracy: 0.6902 - val_loss: 0.6583 - val_accuracy: 0.6781\n",
      "Epoch 32/256\n",
      "186/186 - 1s - loss: 0.6514 - accuracy: 0.6919 - val_loss: 0.6781 - val_accuracy: 0.6452\n",
      "Epoch 33/256\n",
      "186/186 - 1s - loss: 0.6517 - accuracy: 0.6907 - val_loss: 0.6536 - val_accuracy: 0.6851\n",
      "Epoch 34/256\n",
      "186/186 - 1s - loss: 0.6509 - accuracy: 0.6898 - val_loss: 0.7284 - val_accuracy: 0.6290\n",
      "Epoch 35/256\n",
      "186/186 - 1s - loss: 0.6490 - accuracy: 0.6910 - val_loss: 0.6512 - val_accuracy: 0.6875\n",
      "Epoch 36/256\n",
      "186/186 - 1s - loss: 0.6489 - accuracy: 0.6890 - val_loss: 0.6560 - val_accuracy: 0.6790\n",
      "Epoch 37/256\n",
      "186/186 - 2s - loss: 0.6485 - accuracy: 0.6897 - val_loss: 0.6512 - val_accuracy: 0.6876\n",
      "Epoch 38/256\n",
      "186/186 - 1s - loss: 0.6481 - accuracy: 0.6899 - val_loss: 0.6604 - val_accuracy: 0.6799\n",
      "Epoch 39/256\n",
      "186/186 - 1s - loss: 0.6503 - accuracy: 0.6902 - val_loss: 0.6529 - val_accuracy: 0.6860\n",
      "Epoch 40/256\n",
      "186/186 - 1s - loss: 0.6484 - accuracy: 0.6900 - val_loss: 0.6769 - val_accuracy: 0.6647\n",
      "Epoch 41/256\n",
      "186/186 - 1s - loss: 0.6483 - accuracy: 0.6909 - val_loss: 0.6854 - val_accuracy: 0.6501\n",
      "Epoch 42/256\n",
      "186/186 - 1s - loss: 0.6476 - accuracy: 0.6901 - val_loss: 0.6699 - val_accuracy: 0.6691\n",
      "Epoch 43/256\n",
      "186/186 - 1s - loss: 0.6462 - accuracy: 0.6914 - val_loss: 0.6381 - val_accuracy: 0.6937\n",
      "Epoch 44/256\n",
      "186/186 - 1s - loss: 0.6456 - accuracy: 0.6917 - val_loss: 0.6436 - val_accuracy: 0.6914\n",
      "Epoch 45/256\n",
      "186/186 - 1s - loss: 0.6459 - accuracy: 0.6910 - val_loss: 0.6415 - val_accuracy: 0.6913\n",
      "Epoch 46/256\n",
      "186/186 - 1s - loss: 0.6452 - accuracy: 0.6923 - val_loss: 0.6471 - val_accuracy: 0.6813\n",
      "Epoch 47/256\n",
      "186/186 - 1s - loss: 0.6448 - accuracy: 0.6922 - val_loss: 0.7943 - val_accuracy: 0.6103\n",
      "Epoch 48/256\n",
      "186/186 - 1s - loss: 0.6453 - accuracy: 0.6916 - val_loss: 0.6488 - val_accuracy: 0.6844\n",
      "Epoch 49/256\n",
      "186/186 - 1s - loss: 0.6427 - accuracy: 0.6922 - val_loss: 0.6614 - val_accuracy: 0.6682\n",
      "Epoch 50/256\n",
      "186/186 - 1s - loss: 0.6434 - accuracy: 0.6917 - val_loss: 0.6431 - val_accuracy: 0.6836\n",
      "Epoch 51/256\n",
      "186/186 - 1s - loss: 0.6431 - accuracy: 0.6932 - val_loss: 0.6345 - val_accuracy: 0.6977\n",
      "Epoch 52/256\n",
      "186/186 - 1s - loss: 0.6429 - accuracy: 0.6912 - val_loss: 0.6332 - val_accuracy: 0.6968\n",
      "Epoch 53/256\n",
      "186/186 - 1s - loss: 0.6408 - accuracy: 0.6918 - val_loss: 0.6856 - val_accuracy: 0.6732\n",
      "Epoch 54/256\n",
      "186/186 - 1s - loss: 0.6380 - accuracy: 0.6932 - val_loss: 0.6323 - val_accuracy: 0.6925\n",
      "Epoch 55/256\n",
      "186/186 - 1s - loss: 0.6397 - accuracy: 0.6939 - val_loss: 0.6434 - val_accuracy: 0.6847\n",
      "Epoch 56/256\n",
      "186/186 - 1s - loss: 0.6389 - accuracy: 0.6926 - val_loss: 0.6344 - val_accuracy: 0.6969\n",
      "Epoch 57/256\n",
      "186/186 - 1s - loss: 0.6415 - accuracy: 0.6927 - val_loss: 0.6619 - val_accuracy: 0.6665\n",
      "Epoch 58/256\n",
      "186/186 - 1s - loss: 0.6403 - accuracy: 0.6923 - val_loss: 0.6393 - val_accuracy: 0.6957\n",
      "Epoch 59/256\n",
      "186/186 - 1s - loss: 0.6410 - accuracy: 0.6910 - val_loss: 0.6489 - val_accuracy: 0.6826\n",
      "Epoch 60/256\n",
      "186/186 - 1s - loss: 0.6391 - accuracy: 0.6933 - val_loss: 0.6614 - val_accuracy: 0.6753\n",
      "Epoch 61/256\n",
      "186/186 - 1s - loss: 0.6380 - accuracy: 0.6932 - val_loss: 0.6340 - val_accuracy: 0.6974\n",
      "Epoch 62/256\n",
      "186/186 - 1s - loss: 0.6384 - accuracy: 0.6927 - val_loss: 0.6443 - val_accuracy: 0.6812\n",
      "Epoch 63/256\n",
      "186/186 - 2s - loss: 0.6378 - accuracy: 0.6923 - val_loss: 0.6259 - val_accuracy: 0.7000\n",
      "Epoch 64/256\n",
      "186/186 - 1s - loss: 0.6359 - accuracy: 0.6926 - val_loss: 0.6252 - val_accuracy: 0.6977\n",
      "Epoch 65/256\n",
      "186/186 - 1s - loss: 0.6355 - accuracy: 0.6929 - val_loss: 0.6615 - val_accuracy: 0.6720\n",
      "Epoch 66/256\n",
      "186/186 - 1s - loss: 0.6350 - accuracy: 0.6936 - val_loss: 0.6368 - val_accuracy: 0.6821\n",
      "Epoch 67/256\n",
      "186/186 - 1s - loss: 0.6340 - accuracy: 0.6935 - val_loss: 0.6661 - val_accuracy: 0.6626\n",
      "Epoch 68/256\n",
      "186/186 - 1s - loss: 0.6355 - accuracy: 0.6937 - val_loss: 0.6429 - val_accuracy: 0.6817\n",
      "Epoch 69/256\n",
      "186/186 - 1s - loss: 0.6368 - accuracy: 0.6928 - val_loss: 0.6368 - val_accuracy: 0.6876\n",
      "Epoch 70/256\n",
      "186/186 - 2s - loss: 0.6364 - accuracy: 0.6929 - val_loss: 0.6283 - val_accuracy: 0.6976\n",
      "Epoch 71/256\n",
      "186/186 - 2s - loss: 0.6363 - accuracy: 0.6936 - val_loss: 0.6266 - val_accuracy: 0.6968\n",
      "Epoch 72/256\n",
      "186/186 - 1s - loss: 0.6346 - accuracy: 0.6926 - val_loss: 0.6253 - val_accuracy: 0.6984\n",
      "Epoch 73/256\n",
      "186/186 - 1s - loss: 0.6338 - accuracy: 0.6933 - val_loss: 0.7178 - val_accuracy: 0.6274\n",
      "Epoch 74/256\n",
      "186/186 - 2s - loss: 0.6342 - accuracy: 0.6937 - val_loss: 0.6273 - val_accuracy: 0.6978\n",
      "Epoch 75/256\n",
      "186/186 - 1s - loss: 0.6348 - accuracy: 0.6951 - val_loss: 0.6395 - val_accuracy: 0.6855\n",
      "Epoch 76/256\n",
      "186/186 - 1s - loss: 0.6350 - accuracy: 0.6928 - val_loss: 0.6315 - val_accuracy: 0.6923\n",
      "Epoch 77/256\n",
      "186/186 - 1s - loss: 0.6329 - accuracy: 0.6934 - val_loss: 0.6251 - val_accuracy: 0.6965\n",
      "Epoch 78/256\n",
      "186/186 - 1s - loss: 0.6332 - accuracy: 0.6934 - val_loss: 0.6485 - val_accuracy: 0.6781\n",
      "Epoch 79/256\n",
      "186/186 - 1s - loss: 0.6331 - accuracy: 0.6944 - val_loss: 0.6366 - val_accuracy: 0.6910\n",
      "Epoch 80/256\n",
      "186/186 - 1s - loss: 0.6308 - accuracy: 0.6959 - val_loss: 0.6246 - val_accuracy: 0.6946\n",
      "Epoch 81/256\n",
      "186/186 - 1s - loss: 0.6313 - accuracy: 0.6954 - val_loss: 0.6227 - val_accuracy: 0.7003\n",
      "Epoch 82/256\n",
      "186/186 - 1s - loss: 0.6315 - accuracy: 0.6934 - val_loss: 0.6535 - val_accuracy: 0.6743\n",
      "Epoch 83/256\n",
      "186/186 - 2s - loss: 0.6297 - accuracy: 0.6940 - val_loss: 0.6452 - val_accuracy: 0.6777\n",
      "Epoch 84/256\n",
      "186/186 - 1s - loss: 0.6312 - accuracy: 0.6947 - val_loss: 0.6334 - val_accuracy: 0.6939\n",
      "Epoch 85/256\n",
      "186/186 - 1s - loss: 0.6295 - accuracy: 0.6954 - val_loss: 0.6204 - val_accuracy: 0.6987\n",
      "Epoch 86/256\n",
      "186/186 - 1s - loss: 0.6296 - accuracy: 0.6950 - val_loss: 0.6282 - val_accuracy: 0.6929\n",
      "Epoch 87/256\n",
      "186/186 - 1s - loss: 0.6316 - accuracy: 0.6952 - val_loss: 0.6240 - val_accuracy: 0.6952\n",
      "Epoch 88/256\n",
      "186/186 - 1s - loss: 0.6314 - accuracy: 0.6937 - val_loss: 0.7138 - val_accuracy: 0.6412\n",
      "Epoch 89/256\n",
      "186/186 - 1s - loss: 0.6299 - accuracy: 0.6932 - val_loss: 0.6486 - val_accuracy: 0.6742\n",
      "Epoch 90/256\n",
      "186/186 - 1s - loss: 0.6320 - accuracy: 0.6930 - val_loss: 0.6295 - val_accuracy: 0.6883\n",
      "Epoch 91/256\n",
      "186/186 - 1s - loss: 0.6300 - accuracy: 0.6930 - val_loss: 0.6383 - val_accuracy: 0.6804\n",
      "Epoch 92/256\n",
      "186/186 - 1s - loss: 0.6310 - accuracy: 0.6925 - val_loss: 0.6337 - val_accuracy: 0.6816\n",
      "Epoch 93/256\n",
      "186/186 - 1s - loss: 0.6272 - accuracy: 0.6930 - val_loss: 0.6707 - val_accuracy: 0.6432\n",
      "Epoch 94/256\n",
      "186/186 - 1s - loss: 0.6297 - accuracy: 0.6933 - val_loss: 0.6399 - val_accuracy: 0.6876\n",
      "Epoch 95/256\n",
      "186/186 - 1s - loss: 0.6291 - accuracy: 0.6937 - val_loss: 0.6199 - val_accuracy: 0.7018\n",
      "Epoch 96/256\n",
      "186/186 - 1s - loss: 0.6295 - accuracy: 0.6943 - val_loss: 0.6274 - val_accuracy: 0.6926\n",
      "Epoch 97/256\n",
      "186/186 - 1s - loss: 0.6266 - accuracy: 0.6951 - val_loss: 0.6191 - val_accuracy: 0.6987\n",
      "Epoch 98/256\n",
      "186/186 - 1s - loss: 0.6267 - accuracy: 0.6937 - val_loss: 0.6253 - val_accuracy: 0.6908\n",
      "Epoch 99/256\n",
      "186/186 - 2s - loss: 0.6271 - accuracy: 0.6939 - val_loss: 0.6261 - val_accuracy: 0.6881\n",
      "Epoch 100/256\n",
      "186/186 - 2s - loss: 0.6275 - accuracy: 0.6931 - val_loss: 0.6512 - val_accuracy: 0.6662\n",
      "Epoch 101/256\n",
      "186/186 - 1s - loss: 0.6279 - accuracy: 0.6935 - val_loss: 0.6201 - val_accuracy: 0.6992\n",
      "Epoch 102/256\n",
      "186/186 - 1s - loss: 0.6277 - accuracy: 0.6935 - val_loss: 0.6177 - val_accuracy: 0.6997\n",
      "Epoch 103/256\n",
      "186/186 - 1s - loss: 0.6263 - accuracy: 0.6947 - val_loss: 0.6200 - val_accuracy: 0.6974\n",
      "Epoch 104/256\n",
      "186/186 - 2s - loss: 0.6271 - accuracy: 0.6936 - val_loss: 0.6189 - val_accuracy: 0.6965\n",
      "Epoch 105/256\n",
      "186/186 - 1s - loss: 0.6269 - accuracy: 0.6941 - val_loss: 0.6273 - val_accuracy: 0.6923\n",
      "Epoch 106/256\n",
      "186/186 - 2s - loss: 0.6273 - accuracy: 0.6936 - val_loss: 0.6297 - val_accuracy: 0.6857\n",
      "Epoch 107/256\n",
      "186/186 - 1s - loss: 0.6257 - accuracy: 0.6944 - val_loss: 0.6284 - val_accuracy: 0.6883\n",
      "Epoch 108/256\n",
      "186/186 - 1s - loss: 0.6262 - accuracy: 0.6949 - val_loss: 0.6417 - val_accuracy: 0.6726\n",
      "Epoch 109/256\n",
      "186/186 - 1s - loss: 0.6256 - accuracy: 0.6943 - val_loss: 0.6412 - val_accuracy: 0.6677\n",
      "Epoch 110/256\n",
      "186/186 - 1s - loss: 0.6252 - accuracy: 0.6937 - val_loss: 0.6132 - val_accuracy: 0.7009\n",
      "Epoch 111/256\n",
      "186/186 - 1s - loss: 0.6258 - accuracy: 0.6936 - val_loss: 0.6274 - val_accuracy: 0.6866\n",
      "Epoch 112/256\n",
      "186/186 - 1s - loss: 0.6252 - accuracy: 0.6945 - val_loss: 0.6257 - val_accuracy: 0.6929\n",
      "Epoch 113/256\n",
      "186/186 - 1s - loss: 0.6263 - accuracy: 0.6948 - val_loss: 0.6372 - val_accuracy: 0.6753\n",
      "Epoch 114/256\n",
      "186/186 - 1s - loss: 0.6262 - accuracy: 0.6933 - val_loss: 0.6174 - val_accuracy: 0.6969\n",
      "Epoch 115/256\n",
      "186/186 - 1s - loss: 0.6255 - accuracy: 0.6945 - val_loss: 0.6272 - val_accuracy: 0.6892\n",
      "Epoch 116/256\n",
      "186/186 - 1s - loss: 0.6252 - accuracy: 0.6938 - val_loss: 0.6283 - val_accuracy: 0.6869\n",
      "Epoch 117/256\n",
      "186/186 - 1s - loss: 0.6257 - accuracy: 0.6948 - val_loss: 0.6237 - val_accuracy: 0.6935\n",
      "Epoch 118/256\n",
      "186/186 - 1s - loss: 0.6250 - accuracy: 0.6940 - val_loss: 0.6158 - val_accuracy: 0.6994\n",
      "Epoch 119/256\n",
      "186/186 - 1s - loss: 0.6244 - accuracy: 0.6952 - val_loss: 0.6130 - val_accuracy: 0.6986\n",
      "Epoch 120/256\n",
      "186/186 - 1s - loss: 0.6249 - accuracy: 0.6946 - val_loss: 0.6266 - val_accuracy: 0.6892\n",
      "Epoch 121/256\n",
      "186/186 - 1s - loss: 0.6249 - accuracy: 0.6943 - val_loss: 0.6274 - val_accuracy: 0.6918\n",
      "Epoch 122/256\n",
      "186/186 - 1s - loss: 0.6252 - accuracy: 0.6944 - val_loss: 0.6270 - val_accuracy: 0.6919\n",
      "Epoch 123/256\n",
      "186/186 - 1s - loss: 0.6253 - accuracy: 0.6936 - val_loss: 0.6245 - val_accuracy: 0.6907\n",
      "Epoch 124/256\n",
      "186/186 - 1s - loss: 0.6228 - accuracy: 0.6953 - val_loss: 0.6314 - val_accuracy: 0.6915\n",
      "Epoch 125/256\n",
      "186/186 - 2s - loss: 0.6232 - accuracy: 0.6934 - val_loss: 0.6127 - val_accuracy: 0.7044\n",
      "Epoch 126/256\n",
      "186/186 - 1s - loss: 0.6238 - accuracy: 0.6947 - val_loss: 0.6217 - val_accuracy: 0.6935\n",
      "Epoch 127/256\n",
      "186/186 - 1s - loss: 0.6243 - accuracy: 0.6949 - val_loss: 0.6248 - val_accuracy: 0.6913\n",
      "Epoch 128/256\n",
      "186/186 - 1s - loss: 0.6230 - accuracy: 0.6955 - val_loss: 0.6314 - val_accuracy: 0.6876\n",
      "Epoch 129/256\n",
      "186/186 - 1s - loss: 0.6229 - accuracy: 0.6955 - val_loss: 0.6346 - val_accuracy: 0.6830\n",
      "Epoch 130/256\n",
      "186/186 - 1s - loss: 0.6236 - accuracy: 0.6954 - val_loss: 0.6138 - val_accuracy: 0.6985\n",
      "Epoch 131/256\n",
      "186/186 - 1s - loss: 0.6225 - accuracy: 0.6957 - val_loss: 0.6147 - val_accuracy: 0.6959\n",
      "Epoch 132/256\n",
      "186/186 - 2s - loss: 0.6218 - accuracy: 0.6949 - val_loss: 0.6129 - val_accuracy: 0.7008\n",
      "Epoch 133/256\n",
      "186/186 - 2s - loss: 0.6215 - accuracy: 0.6964 - val_loss: 0.6211 - val_accuracy: 0.6930\n",
      "Epoch 134/256\n",
      "186/186 - 1s - loss: 0.6215 - accuracy: 0.6955 - val_loss: 0.6197 - val_accuracy: 0.6929\n",
      "Epoch 135/256\n",
      "186/186 - 1s - loss: 0.6217 - accuracy: 0.6957 - val_loss: 0.6151 - val_accuracy: 0.6991\n",
      "Epoch 136/256\n",
      "186/186 - 1s - loss: 0.6231 - accuracy: 0.6959 - val_loss: 0.6122 - val_accuracy: 0.6982\n",
      "Epoch 137/256\n",
      "186/186 - 1s - loss: 0.6245 - accuracy: 0.6945 - val_loss: 0.6160 - val_accuracy: 0.6965\n",
      "Epoch 138/256\n",
      "186/186 - 1s - loss: 0.6231 - accuracy: 0.6942 - val_loss: 0.6296 - val_accuracy: 0.6863\n",
      "Epoch 139/256\n",
      "186/186 - 1s - loss: 0.6232 - accuracy: 0.6940 - val_loss: 0.6207 - val_accuracy: 0.6953\n",
      "Epoch 140/256\n",
      "186/186 - 1s - loss: 0.6235 - accuracy: 0.6953 - val_loss: 0.6393 - val_accuracy: 0.6755\n",
      "Epoch 141/256\n",
      "186/186 - 1s - loss: 0.6219 - accuracy: 0.6936 - val_loss: 0.6161 - val_accuracy: 0.6979\n",
      "Epoch 142/256\n",
      "186/186 - 1s - loss: 0.6210 - accuracy: 0.6956 - val_loss: 0.6254 - val_accuracy: 0.6860\n",
      "Epoch 143/256\n",
      "186/186 - 1s - loss: 0.6216 - accuracy: 0.6969 - val_loss: 0.6133 - val_accuracy: 0.6996\n",
      "Epoch 144/256\n",
      "186/186 - 1s - loss: 0.6214 - accuracy: 0.6945 - val_loss: 0.6117 - val_accuracy: 0.7039\n",
      "Epoch 145/256\n",
      "186/186 - 1s - loss: 0.6226 - accuracy: 0.6957 - val_loss: 0.6330 - val_accuracy: 0.6816\n",
      "Epoch 146/256\n",
      "186/186 - 1s - loss: 0.6204 - accuracy: 0.6964 - val_loss: 0.6296 - val_accuracy: 0.6836\n",
      "Epoch 147/256\n",
      "186/186 - 1s - loss: 0.6223 - accuracy: 0.6954 - val_loss: 0.6280 - val_accuracy: 0.6858\n",
      "Epoch 148/256\n",
      "186/186 - 1s - loss: 0.6227 - accuracy: 0.6943 - val_loss: 0.6130 - val_accuracy: 0.6992\n",
      "Epoch 149/256\n",
      "186/186 - 1s - loss: 0.6223 - accuracy: 0.6937 - val_loss: 0.6250 - val_accuracy: 0.6890\n",
      "Epoch 150/256\n",
      "186/186 - 1s - loss: 0.6217 - accuracy: 0.6944 - val_loss: 0.6149 - val_accuracy: 0.6957\n",
      "Epoch 151/256\n",
      "186/186 - 1s - loss: 0.6210 - accuracy: 0.6938 - val_loss: 0.6246 - val_accuracy: 0.6869\n",
      "Epoch 152/256\n",
      "186/186 - 1s - loss: 0.6225 - accuracy: 0.6940 - val_loss: 0.6216 - val_accuracy: 0.6927\n",
      "Epoch 153/256\n",
      "186/186 - 1s - loss: 0.6219 - accuracy: 0.6946 - val_loss: 0.6190 - val_accuracy: 0.6949\n",
      "Epoch 154/256\n",
      "186/186 - 1s - loss: 0.6242 - accuracy: 0.6938 - val_loss: 0.6146 - val_accuracy: 0.7001\n",
      "Epoch 155/256\n",
      "186/186 - 1s - loss: 0.6227 - accuracy: 0.6942 - val_loss: 0.6214 - val_accuracy: 0.6896\n",
      "Epoch 156/256\n",
      "186/186 - 1s - loss: 0.6211 - accuracy: 0.6948 - val_loss: 0.6303 - val_accuracy: 0.6850\n",
      "Epoch 157/256\n",
      "186/186 - 1s - loss: 0.6212 - accuracy: 0.6947 - val_loss: 0.6098 - val_accuracy: 0.7014\n",
      "Epoch 158/256\n",
      "186/186 - 1s - loss: 0.6201 - accuracy: 0.6948 - val_loss: 0.6172 - val_accuracy: 0.6962\n",
      "Epoch 159/256\n",
      "186/186 - 1s - loss: 0.6228 - accuracy: 0.6941 - val_loss: 0.6227 - val_accuracy: 0.6931\n",
      "Epoch 160/256\n",
      "186/186 - 1s - loss: 0.6210 - accuracy: 0.6940 - val_loss: 0.6116 - val_accuracy: 0.6962\n",
      "Epoch 161/256\n",
      "186/186 - 1s - loss: 0.6203 - accuracy: 0.6949 - val_loss: 0.6186 - val_accuracy: 0.6922\n",
      "Epoch 162/256\n",
      "186/186 - 1s - loss: 0.6210 - accuracy: 0.6954 - val_loss: 0.6102 - val_accuracy: 0.7041\n",
      "Epoch 163/256\n",
      "186/186 - 1s - loss: 0.6205 - accuracy: 0.6957 - val_loss: 0.6344 - val_accuracy: 0.6748\n",
      "Epoch 164/256\n",
      "186/186 - 1s - loss: 0.6206 - accuracy: 0.6953 - val_loss: 0.6223 - val_accuracy: 0.6924\n",
      "Epoch 165/256\n",
      "186/186 - 1s - loss: 0.6203 - accuracy: 0.6955 - val_loss: 0.6255 - val_accuracy: 0.6880\n",
      "Epoch 166/256\n",
      "186/186 - 2s - loss: 0.6189 - accuracy: 0.6968 - val_loss: 0.6220 - val_accuracy: 0.6911\n",
      "Epoch 167/256\n",
      "186/186 - 2s - loss: 0.6184 - accuracy: 0.6953 - val_loss: 0.6191 - val_accuracy: 0.6921\n",
      "Epoch 168/256\n",
      "186/186 - 1s - loss: 0.6211 - accuracy: 0.6951 - val_loss: 0.6325 - val_accuracy: 0.6773\n",
      "Epoch 169/256\n",
      "186/186 - 2s - loss: 0.6192 - accuracy: 0.6948 - val_loss: 0.6123 - val_accuracy: 0.6979\n",
      "Epoch 170/256\n",
      "186/186 - 2s - loss: 0.6178 - accuracy: 0.6956 - val_loss: 0.6244 - val_accuracy: 0.6885\n",
      "Epoch 171/256\n",
      "186/186 - 1s - loss: 0.6193 - accuracy: 0.6955 - val_loss: 0.6155 - val_accuracy: 0.6937\n",
      "Epoch 172/256\n",
      "186/186 - 1s - loss: 0.6197 - accuracy: 0.6958 - val_loss: 0.6083 - val_accuracy: 0.7053\n",
      "Epoch 173/256\n",
      "186/186 - 1s - loss: 0.6203 - accuracy: 0.6955 - val_loss: 0.6170 - val_accuracy: 0.6929\n",
      "Epoch 174/256\n",
      "186/186 - 1s - loss: 0.6192 - accuracy: 0.6947 - val_loss: 0.6197 - val_accuracy: 0.6869\n",
      "Epoch 175/256\n",
      "186/186 - 2s - loss: 0.6187 - accuracy: 0.6960 - val_loss: 0.6473 - val_accuracy: 0.6746\n",
      "Epoch 176/256\n",
      "186/186 - 2s - loss: 0.6207 - accuracy: 0.6952 - val_loss: 0.6147 - val_accuracy: 0.6953\n",
      "Epoch 177/256\n",
      "186/186 - 2s - loss: 0.6214 - accuracy: 0.6943 - val_loss: 0.6092 - val_accuracy: 0.7017\n",
      "Epoch 178/256\n",
      "186/186 - 1s - loss: 0.6185 - accuracy: 0.6958 - val_loss: 0.6393 - val_accuracy: 0.6716\n",
      "Epoch 179/256\n",
      "186/186 - 1s - loss: 0.6205 - accuracy: 0.6952 - val_loss: 0.6135 - val_accuracy: 0.6999\n",
      "Epoch 180/256\n",
      "186/186 - 1s - loss: 0.6189 - accuracy: 0.6955 - val_loss: 0.6138 - val_accuracy: 0.6970\n",
      "Epoch 181/256\n",
      "186/186 - 1s - loss: 0.6185 - accuracy: 0.6956 - val_loss: 0.6178 - val_accuracy: 0.6904\n",
      "Epoch 182/256\n",
      "186/186 - 1s - loss: 0.6199 - accuracy: 0.6952 - val_loss: 0.6215 - val_accuracy: 0.6867\n",
      "Epoch 183/256\n",
      "186/186 - 1s - loss: 0.6203 - accuracy: 0.6938 - val_loss: 0.6176 - val_accuracy: 0.6956\n",
      "Epoch 184/256\n",
      "186/186 - 1s - loss: 0.6183 - accuracy: 0.6951 - val_loss: 0.6107 - val_accuracy: 0.7032\n",
      "Epoch 185/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6959 - val_loss: 0.6270 - val_accuracy: 0.6822\n",
      "Epoch 186/256\n",
      "186/186 - 1s - loss: 0.6181 - accuracy: 0.6943 - val_loss: 0.6045 - val_accuracy: 0.7045\n",
      "Epoch 187/256\n",
      "186/186 - 1s - loss: 0.6213 - accuracy: 0.6944 - val_loss: 0.6147 - val_accuracy: 0.6946\n",
      "Epoch 188/256\n",
      "186/186 - 1s - loss: 0.6191 - accuracy: 0.6948 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 189/256\n",
      "186/186 - 1s - loss: 0.6201 - accuracy: 0.6950 - val_loss: 0.6074 - val_accuracy: 0.7029\n",
      "Epoch 190/256\n",
      "186/186 - 1s - loss: 0.6193 - accuracy: 0.6939 - val_loss: 0.6188 - val_accuracy: 0.6913\n",
      "Epoch 191/256\n",
      "186/186 - 1s - loss: 0.6199 - accuracy: 0.6947 - val_loss: 0.6355 - val_accuracy: 0.6774\n",
      "Epoch 192/256\n",
      "186/186 - 1s - loss: 0.6192 - accuracy: 0.6952 - val_loss: 0.6410 - val_accuracy: 0.6815\n",
      "Epoch 193/256\n",
      "186/186 - 1s - loss: 0.6180 - accuracy: 0.6949 - val_loss: 0.6168 - val_accuracy: 0.6953\n",
      "Epoch 194/256\n",
      "186/186 - 1s - loss: 0.6189 - accuracy: 0.6949 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 195/256\n",
      "186/186 - 1s - loss: 0.6198 - accuracy: 0.6948 - val_loss: 0.6104 - val_accuracy: 0.7013\n",
      "Epoch 196/256\n",
      "186/186 - 1s - loss: 0.6188 - accuracy: 0.6972 - val_loss: 0.6066 - val_accuracy: 0.7012\n",
      "Epoch 197/256\n",
      "186/186 - 1s - loss: 0.6208 - accuracy: 0.6936 - val_loss: 0.6161 - val_accuracy: 0.6982\n",
      "Epoch 198/256\n",
      "186/186 - 1s - loss: 0.6186 - accuracy: 0.6948 - val_loss: 0.6095 - val_accuracy: 0.7007\n",
      "Epoch 199/256\n",
      "186/186 - 1s - loss: 0.6179 - accuracy: 0.6946 - val_loss: 0.6060 - val_accuracy: 0.7045\n",
      "Epoch 200/256\n",
      "186/186 - 1s - loss: 0.6180 - accuracy: 0.6948 - val_loss: 0.6262 - val_accuracy: 0.6850\n",
      "Epoch 201/256\n",
      "186/186 - 1s - loss: 0.6196 - accuracy: 0.6927 - val_loss: 0.6243 - val_accuracy: 0.6846\n",
      "Epoch 202/256\n",
      "186/186 - 1s - loss: 0.6176 - accuracy: 0.6949 - val_loss: 0.6054 - val_accuracy: 0.7034\n",
      "Epoch 203/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6952 - val_loss: 0.6162 - val_accuracy: 0.6970\n",
      "Epoch 204/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6957 - val_loss: 0.6153 - val_accuracy: 0.6949\n",
      "Epoch 205/256\n",
      "186/186 - 1s - loss: 0.6194 - accuracy: 0.6948 - val_loss: 0.6119 - val_accuracy: 0.6987\n",
      "Epoch 206/256\n",
      "186/186 - 1s - loss: 0.6197 - accuracy: 0.6939 - val_loss: 0.6566 - val_accuracy: 0.6536\n",
      "Epoch 207/256\n",
      "186/186 - 1s - loss: 0.6187 - accuracy: 0.6951 - val_loss: 0.6277 - val_accuracy: 0.6874\n",
      "Epoch 208/256\n",
      "186/186 - 1s - loss: 0.6178 - accuracy: 0.6954 - val_loss: 0.6133 - val_accuracy: 0.6916\n",
      "Epoch 209/256\n",
      "186/186 - 1s - loss: 0.6174 - accuracy: 0.6958 - val_loss: 0.6093 - val_accuracy: 0.6985\n",
      "Epoch 210/256\n",
      "186/186 - 1s - loss: 0.6179 - accuracy: 0.6947 - val_loss: 0.6129 - val_accuracy: 0.6980\n",
      "Epoch 211/256\n",
      "186/186 - 1s - loss: 0.6174 - accuracy: 0.6946 - val_loss: 0.6129 - val_accuracy: 0.6967\n",
      "Epoch 212/256\n",
      "186/186 - 1s - loss: 0.6170 - accuracy: 0.6955 - val_loss: 0.6200 - val_accuracy: 0.6904\n",
      "Epoch 213/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6950 - val_loss: 0.6082 - val_accuracy: 0.7023\n",
      "Epoch 214/256\n",
      "186/186 - 1s - loss: 0.6176 - accuracy: 0.6951 - val_loss: 0.6249 - val_accuracy: 0.6806\n",
      "Epoch 215/256\n",
      "186/186 - 1s - loss: 0.6173 - accuracy: 0.6962 - val_loss: 0.6137 - val_accuracy: 0.6965\n",
      "Epoch 216/256\n",
      "186/186 - 1s - loss: 0.6185 - accuracy: 0.6953 - val_loss: 0.6160 - val_accuracy: 0.6973\n",
      "Epoch 217/256\n",
      "186/186 - 2s - loss: 0.6181 - accuracy: 0.6947 - val_loss: 0.6094 - val_accuracy: 0.7024\n",
      "Epoch 218/256\n",
      "186/186 - 1s - loss: 0.6175 - accuracy: 0.6956 - val_loss: 0.6099 - val_accuracy: 0.7015\n",
      "Epoch 219/256\n",
      "186/186 - 1s - loss: 0.6184 - accuracy: 0.6957 - val_loss: 0.6085 - val_accuracy: 0.7034\n",
      "Epoch 220/256\n",
      "186/186 - 1s - loss: 0.6188 - accuracy: 0.6956 - val_loss: 0.6080 - val_accuracy: 0.7060\n",
      "Epoch 221/256\n",
      "186/186 - 1s - loss: 0.6183 - accuracy: 0.6957 - val_loss: 0.6062 - val_accuracy: 0.7048\n",
      "Epoch 222/256\n",
      "186/186 - 1s - loss: 0.6191 - accuracy: 0.6956 - val_loss: 0.6050 - val_accuracy: 0.7074\n",
      "Epoch 223/256\n",
      "186/186 - 1s - loss: 0.6189 - accuracy: 0.6959 - val_loss: 0.6223 - val_accuracy: 0.6925\n",
      "Epoch 224/256\n",
      "186/186 - 1s - loss: 0.6197 - accuracy: 0.6946 - val_loss: 0.6174 - val_accuracy: 0.6895\n",
      "Epoch 225/256\n",
      "186/186 - 1s - loss: 0.6197 - accuracy: 0.6942 - val_loss: 0.6153 - val_accuracy: 0.6996\n",
      "Epoch 226/256\n",
      "186/186 - 1s - loss: 0.6192 - accuracy: 0.6952 - val_loss: 0.6103 - val_accuracy: 0.6972\n",
      "Epoch 227/256\n",
      "186/186 - 1s - loss: 0.6188 - accuracy: 0.6951 - val_loss: 0.6092 - val_accuracy: 0.7013\n",
      "Epoch 228/256\n",
      "186/186 - 1s - loss: 0.6184 - accuracy: 0.6959 - val_loss: 0.6085 - val_accuracy: 0.7003\n",
      "Epoch 229/256\n",
      "186/186 - 1s - loss: 0.6178 - accuracy: 0.6962 - val_loss: 0.6140 - val_accuracy: 0.7018\n",
      "Epoch 230/256\n",
      "186/186 - 1s - loss: 0.6175 - accuracy: 0.6963 - val_loss: 0.6205 - val_accuracy: 0.6898\n",
      "Epoch 231/256\n",
      "186/186 - 1s - loss: 0.6167 - accuracy: 0.6966 - val_loss: 0.6112 - val_accuracy: 0.7018\n",
      "Epoch 232/256\n",
      "186/186 - 1s - loss: 0.6161 - accuracy: 0.6963 - val_loss: 0.6148 - val_accuracy: 0.6920\n",
      "Epoch 233/256\n",
      "186/186 - 1s - loss: 0.6181 - accuracy: 0.6961 - val_loss: 0.6112 - val_accuracy: 0.7015\n",
      "Epoch 234/256\n",
      "186/186 - 1s - loss: 0.6182 - accuracy: 0.6959 - val_loss: 0.6369 - val_accuracy: 0.6778\n",
      "Epoch 235/256\n",
      "186/186 - 1s - loss: 0.6158 - accuracy: 0.6971 - val_loss: 0.6265 - val_accuracy: 0.6856\n",
      "Epoch 236/256\n",
      "186/186 - 1s - loss: 0.6158 - accuracy: 0.6973 - val_loss: 0.6148 - val_accuracy: 0.6991\n",
      "Epoch 237/256\n",
      "186/186 - 1s - loss: 0.6175 - accuracy: 0.6964 - val_loss: 0.6114 - val_accuracy: 0.7007\n",
      "Epoch 238/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6958 - val_loss: 0.6384 - val_accuracy: 0.6744\n",
      "Epoch 239/256\n",
      "186/186 - 1s - loss: 0.6175 - accuracy: 0.6953 - val_loss: 0.6139 - val_accuracy: 0.6982\n",
      "Epoch 240/256\n",
      "186/186 - 1s - loss: 0.6173 - accuracy: 0.6956 - val_loss: 0.6342 - val_accuracy: 0.6809\n",
      "Epoch 241/256\n",
      "186/186 - 1s - loss: 0.6182 - accuracy: 0.6952 - val_loss: 0.6041 - val_accuracy: 0.7049\n",
      "Epoch 242/256\n",
      "186/186 - 1s - loss: 0.6169 - accuracy: 0.6952 - val_loss: 0.6163 - val_accuracy: 0.6914\n",
      "Epoch 243/256\n",
      "186/186 - 1s - loss: 0.6183 - accuracy: 0.6940 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
      "Epoch 244/256\n",
      "186/186 - 1s - loss: 0.6179 - accuracy: 0.6951 - val_loss: 0.6126 - val_accuracy: 0.6953\n",
      "Epoch 245/256\n",
      "186/186 - 1s - loss: 0.6160 - accuracy: 0.6951 - val_loss: 0.6316 - val_accuracy: 0.6792\n",
      "Epoch 246/256\n",
      "186/186 - 1s - loss: 0.6188 - accuracy: 0.6942 - val_loss: 0.6179 - val_accuracy: 0.6908\n",
      "Epoch 247/256\n",
      "186/186 - 1s - loss: 0.6180 - accuracy: 0.6953 - val_loss: 0.6088 - val_accuracy: 0.7013\n",
      "Epoch 248/256\n",
      "186/186 - 1s - loss: 0.6177 - accuracy: 0.6960 - val_loss: 0.6209 - val_accuracy: 0.6906\n",
      "Epoch 249/256\n",
      "186/186 - 1s - loss: 0.6174 - accuracy: 0.6958 - val_loss: 0.6031 - val_accuracy: 0.7053\n",
      "Epoch 250/256\n",
      "186/186 - 1s - loss: 0.6195 - accuracy: 0.6940 - val_loss: 0.6163 - val_accuracy: 0.6940\n",
      "Epoch 251/256\n",
      "186/186 - 1s - loss: 0.6191 - accuracy: 0.6944 - val_loss: 0.6270 - val_accuracy: 0.6831\n",
      "Epoch 252/256\n",
      "186/186 - 1s - loss: 0.6191 - accuracy: 0.6942 - val_loss: 0.6209 - val_accuracy: 0.6926\n",
      "Epoch 253/256\n",
      "186/186 - 1s - loss: 0.6179 - accuracy: 0.6941 - val_loss: 0.6347 - val_accuracy: 0.6776\n",
      "Epoch 254/256\n",
      "186/186 - 1s - loss: 0.6179 - accuracy: 0.6956 - val_loss: 0.6069 - val_accuracy: 0.7030\n",
      "Epoch 255/256\n",
      "186/186 - 1s - loss: 0.6159 - accuracy: 0.6951 - val_loss: 0.6115 - val_accuracy: 0.7009\n",
      "Epoch 256/256\n",
      "186/186 - 1s - loss: 0.6178 - accuracy: 0.6949 - val_loss: 0.6074 - val_accuracy: 0.7011\n",
      "Wall time: 6min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x27e330c6c40>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#init=uniform\n",
    "#TAKE IT Branda:\n",
    "neuralNetwork = Sequential()\n",
    "neuralNetwork.add(Dense(1536, activation='relu', input_dim=FEATURE_SIZE * 3, kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.5))\n",
    "#neuralNetwork.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "#neuralNetwork.add(Dropout(0.5))\n",
    "neuralNetwork.add(Dense(256, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.3))\n",
    "neuralNetwork.add(Dense(64, activation='relu', kernel_regularizer=l2(0.003)))\n",
    "neuralNetwork.add(BatchNormalization())\n",
    "neuralNetwork.add(Dropout(0.2))\n",
    "neuralNetwork.add(Dense(1, activation='sigmoid'))\n",
    "#neuralNetwork.add(BatchNormalization())\n",
    "\n",
    "neuralNetwork.compile(loss = keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "neuralNetwork.summary()\n",
    "\n",
    "#fit the network to (for now the un-sparse matrix)\n",
    "neuralNetwork.fit(data_x, data_y, epochs=256, batch_size=1024, verbose=2, validation_split = 0.2, callbacks=callbacks)\n",
    "\n",
    "#evaluation of the network prediction\n",
    "#predict_test_nn = neuralNetwork.predict(df_test_onehot_nn)\n",
    "#predict_train_nn = neuralNetwork.predict(df_train_onehot_nn)\n",
    "#print(predict_test_nn)\n",
    "#print(predict_train_nn)\n",
    "#predict_test_nn = (predict_test_nn >= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Layers:                         F.      Epoch   Val_Acc\n",
    "4096-4096-1024-256-64-1         2048    64      0.7962\n",
    "4096-4096-1024-256-64-1         2048    128     0.8043\n",
    "6144-4096-2048-1024-256-64-1    2048    64      0.7943\n",
    "6144-4096-2048-1024-256-64-1    2048    256     0.8044\n",
    "4096-4096-1024-256-64-1         2048    64      0.8329 #using 0.1 initial dropout (score was super bad)\n",
    "4096-4096-1024-256-64-1         2048    64      0.7961 #0.5 Drop except for last 0.2\n",
    "6144-1024-256-64-1              2048    128     0.8031 #check the SVD value and retry again\n",
    "8162-256-64-1                   3072    256     0.732\n",
    "4096-256-64-1                   1024    128     0.718\n",
    "4096-256-64-1                   2048    64      0.804\n",
    "4096-256-64-1                   3072    128     0.805\n",
    "8192-256-64-1                   3072    64      0.805\n",
    "\n",
    "\n",
    "Lets try to find out what dim-red. is needed: 1024-0.5-64-0.2-1 32e\n",
    "Factor:         Val_Acc:\n",
    "3072            0.725\n",
    "2048            0.796 #wtf is this even.\n",
    "1024            0.712\n",
    "\n",
    "\n",
    "Using the Max-Pooling-Avg: VGG16\n",
    "1024-256-64-1                   512     128     0.6990\n",
    "\n",
    "Using the Max-Pooling-Avg: InceptionResNet V2\n",
    "2048-512-64-1                   512     128     0.511"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're at entry: 0 0\n",
      "we're at entry: 0 10000\n",
      "we're at entry: 0 20000\n",
      "we're at entry: 0 30000\n",
      "we're at entry: 0 40000\n",
      "we're at entry: 0 50000\n",
      "we're at entry: 1 0\n",
      "we're at entry: 1 10000\n",
      "we're at entry: 1 20000\n",
      "we're at entry: 1 30000\n",
      "we're at entry: 1 40000\n",
      "we're at entry: 1 50000\n",
      "we're at entry: 2 0\n",
      "we're at entry: 2 10000\n",
      "we're at entry: 2 20000\n",
      "we're at entry: 2 30000\n",
      "we're at entry: 2 40000\n",
      "we're at entry: 2 50000\n"
     ]
    }
   ],
   "source": [
    "test_trip = np.loadtxt(\"test_triplets.txt\").astype('int')\n",
    "test_nn_x = np.zeros((59544,FEATURE_SIZE * 3))\n",
    "for i in range(0,3):\n",
    "        for j in range(0,59544):\n",
    "            test_nn_x[j,range(i*FEATURE_SIZE , FEATURE_SIZE*(i+1))] = load_features(test_trip[j,i].astype('int'))\n",
    "            if j%10000==False:\n",
    "                print(\"we're at entry:\", i, j)\n",
    "\n",
    "predict_test_nn = neuralNetwork.predict(test_nn_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% gotta try some shit:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "export = np.array(predict_test_nn>0.5).astype('int')\n",
    "np.savetxt(\"submission_retrain_v14.txt\",np.round(export,decimals=0), fmt='%i')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% export prediction:\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}